{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Build Interactive LLM Agents with Tools**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **15** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll explore the powerful capabilities of tool calling in large language models (LLMs) to build advanced AI agents that can dynamically interact with users. Using the LangChain framework, youâ€™ll learn how to build an interactive agent that responds to user queries by selecting and executing the right function at the right time. This hands-on approach will help you understand how LLMs can be extended with real-world functionality, bridging natural language understanding with dynamic, tool-based actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "- [Objectives](#Objectives)\n",
    "- [Setup](#Setup)\n",
    "    - [Installing Required Libraries](#Installing-Required-Libraries)\n",
    "    - [Importing Required Libraries](#Importing-Required-Libraries)\n",
    "- [Creating Custom Tools with LangChain](#Creating-Custom-Tools-with-LangChain)\n",
    "    - [Anatomy of a tool](#Anatomy-of-a-tool)\n",
    "    - [Key components](#Key-components)\n",
    "    - [Defining an add function](#Defining-an-add-function)\n",
    "    - [Add tools to the LLM](#Add-tools-to-the-LLM)\n",
    "    - [Create more Tools](#Create-more-tools)\n",
    "    - [Testing the functions](#Testing-the-functions)\n",
    "    - [Add new tools to LLM](#Add-new-tools-to-LLM)\n",
    "- [Interacting with the Model](#Interacting-with-the-Model)\n",
    "    - [Craft the user query](#Craft-the-user-query)\n",
    "    - [Invoke the model](#Invoke-the-model)\n",
    "    - [Parse tool calls](#Parse-tool-calls)\n",
    "    - [Invoke the tool](#Invoke-the-Tool)\n",
    "    - [Generate a final answer from chat history](#Generate-a-final-answer-from-chat-history)\n",
    "- [Building an Agent](#Building-an-Agent)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Exercises](#Exercises)\n",
    "    - [Exercise 1: Create a New Tool](#Exercise-1:-Create-a-new-tool)\n",
    "    - [Exercise 2: Tool Calling with an LLM](#Exercise-2:-Tool-calling-with-an-LLM)\n",
    "    - [Exercise 3: Create a tip calculating agent](#Exercise-3:-Create-a-tip-calculating-agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Initialize a chat model for tool interactions\n",
    " - Define and bind custom tools to the LLM for expanded functionality\n",
    " - Use mapping dictionaries for dynamic function calls\n",
    " - Extract tool names and functions for precise function calls\n",
    " - Build agent classes that manage the entire tool-calling process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "*   [`langchain`](https://python.langchain.com/docs/introduction/) is the framework you will build the agent on.\n",
    "*   [`langchain-openai`](https://pypi.org/project/langchain-openai/) is a partner package of LangChain and integrates OpenAI LLMs to the framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed langchain-0.3.25 langchain-core-0.3.79 langchain-text-splitters-0.3.11 langsmith-0.3.45 orjson-3.11.4 requests-toolbelt-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed jiter-0.11.1 langchain-openai-0.3.19 openai-1.109.1 regex-2025.10.23 tiktoken-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain===0.3.25 | tail -n 1\n",
    "%pip install langchain-openai===0.3.19 | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "Recommendation:Import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the language model that will power your tool calling capabilities. This code sets up a GPT-4o-mini model using the OpenAI provider through LangChain's interface, which you'll use to process queries and decide which tools to call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Disclaimer\n",
    "\n",
    "This lab uses LLMs provided by Watsonx.ai and OpenAI. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to configure your own API keys. Please note that using your own API keys means that you will incur personal charges. \n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API keys. This lab uses `ChatOpenAI` and `ChatWatsonx` modules from `langchain`. Both configurations are shown below with instructions. **Replace all instances** of both modules with the completed modules below throughout the lab. **DO NOT** run the cell below if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE IF YOU ARE NOT RUNNING LOCALLY\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import ChatWatsonx\n",
    "openai_llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    api_key = \"your openai api key here\",\n",
    ")\n",
    "watsonx_llm = ChatWatsonx(\n",
    "    model_id=\"ibm/granite-3-2-8b-instruct\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"your project id associated with the API key\",\n",
    "    api_key=\"your watsonx.ai api key here\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Tools with LangChain\n",
    "\n",
    "### Anatomy of a tool\n",
    "\n",
    "Let's provide the basic building blocks of a tool, consider the following tool:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def tool_name(input_param: input_type) -> output_type:\n",
    "   \"\"\"\n",
    "   Clear description of what the tool does.\n",
    "   \n",
    "   Args:\n",
    "       input_param (input_type): Description of this parameter\n",
    "   \n",
    "   Returns:\n",
    "       output_type: Description of what is returned\n",
    "   \"\"\"\n",
    "   # Function implementation\n",
    "   result = process(input_param)\n",
    "   return result\n",
    "```\n",
    "\n",
    "### Key components\n",
    "\n",
    "You'll use the following key components\n",
    "\n",
    "**@tool decorator**\n",
    "   - Registers the function with LangChain\n",
    "   - Creates tool attributes (.name, .description, .func)\n",
    "   - Generates JSON schema for validation\n",
    "   - Transforms regular functions into callable tools\n",
    "\n",
    "**Function name**\n",
    "   - Used by LLM to select appropriate tool\n",
    "   - Used as reference in chains and tool mappings\n",
    "   - Appears in tool call logs for debugging\n",
    "   - Should clearly indicate the tool's purpose\n",
    "\n",
    "**Type annotations**\n",
    "   - Enable automatic input validation\n",
    "   - Create schema for parameters\n",
    "   - Allow proper serialization of inputs/outputs\n",
    "   - Help LLM understand required input formats\n",
    "\n",
    "**Docstring**\n",
    "   - Provides context for the LLM to decide when to use the tool\n",
    "   - Documents parameter requirements\n",
    "   - Explains expected outputs and behavior\n",
    "   - Is critical for tool selection by the LLM\n",
    "\n",
    "6. **Implementation**\n",
    "   - Executes the actual operation\n",
    "   - Handles errors appropriately\n",
    "   - Returns properly formatted results\n",
    "   - Should be efficient and robust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an add function\n",
    "\n",
    "Now use this tool framework to create a custom tool that enables the LLM to perform basic addition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add a and b.\n",
    "    \n",
    "    Args:\n",
    "        a (int): first integer to be added\n",
    "        b (int): second integer to be added\n",
    "\n",
    "    Return:\n",
    "        int: sum of a and b\n",
    "    \"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decorator wraps the `add()` function in LangChain's predefined tool schema. See more about defining custom LangChain tools [here](https://python.langchain.com/docs/how_to/custom_tools/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add tools to the LLM\n",
    "\n",
    "Let's connect and bind the function to the chat model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `bind_tools(tools)` method to connect a list of tools to the LLM for use. From now on, whenever the call is invoked, the model (with tools) will recognize and use the add tool whenever it needs to compute a sum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create more tools\n",
    "\n",
    "Let's create some more basic arithmetic tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def subtract(a: int, b:int) -> int:\n",
    "    \"\"\"Subtract b from a.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b:int) -> int:\n",
    "    \"\"\"Multiply a and b.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the functions\n",
    "\n",
    "Let's setup a way to test your tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_map = {\n",
    "    \"add\": add, \n",
    "    \"subtract\": subtract,\n",
    "    \"multiply\": multiply\n",
    "}\n",
    "\n",
    "input_ = {\n",
    "    \"a\": 1,\n",
    "    \"b\": 2\n",
    "}\n",
    "\n",
    "tool_map[\"add\"].invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LangChain's built in `.invoke(inputs)` method, you can test each tool built with dynamic inputs.Test each tool with the preceding code block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new tools to LLM\n",
    "\n",
    "Let's add all three tools to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, subtract, multiply]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can the same method to bind tools to the LLM, enabling more arithmetic capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Craft the user query\n",
    "\n",
    "Now that you've setup an LLM with basic tool integrations, it's time to introduce user queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is 3 + 2?\"\n",
    "chat_history = [HumanMessage(content=query)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First,setup the question (user query). Then,initialize a `chat_history` array that will contain the entire conversation between user and LLM. In this chat history, you insert the `query` in a `HumanMessage` wrapper that tells LangChain and the model: \"This message came from the user.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the model\n",
    "\n",
    "Now let's run the model with the context (chat history) that contains the user query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "response_1 = llm_with_tools.invoke(chat_history)\n",
    "chat_history.append(response_1)\n",
    "\n",
    "print(type(response_1))\n",
    "#print(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `invoke(inputs)` method, you get a response from the model. You add the response to the chat history. The code block also prints out the type of the response which is the `AIMessage` class from LangChain. Uncomment the second print statement and read through the fields of the `AIMessage` response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse tool calls\n",
    "\n",
    "Now that you have the response from the model, you can parse the response for tool calling instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool name:\n",
      "add\n",
      "tool args:\n",
      "{'a': 3, 'b': 2}\n",
      "tool call ID:\n",
      "call_QKBCFNEVRjQ1gcaHq6PkaLGB\n"
     ]
    }
   ],
   "source": [
    "tool_calls_1 = response_1.tool_calls\n",
    "\n",
    "tool_1_name = tool_calls_1[0][\"name\"]\n",
    "tool_1_args = tool_calls_1[0][\"args\"]\n",
    "tool_call_1_id = tool_calls_1[0][\"id\"]\n",
    "\n",
    "print(f'tool name:\\n{tool_1_name}')\n",
    "print(f'tool args:\\n{tool_1_args}')\n",
    "print(f'tool call ID:\\n{tool_call_1_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extracting the `name` from the first call gives the name of the tool to use.\n",
    "    - `add` in this case\n",
    "- Extracting the `args` gives the inputs to pass into the tool.\n",
    "    - `{a: 3, b: 2}` in this case\n",
    "- Extracting the `id` gives the unique identifier for the tool call\n",
    "    - The ID will be different each time, linking tool calls to their respective responses\n",
    "    - Crucial in differentiating calls to the same tool and parallel tool calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the Tool\n",
    "\n",
    "Given the tool call details from the LLM, invoke the correct tool with the correct arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='5' tool_call_id='call_QKBCFNEVRjQ1gcaHq6PkaLGB'\n"
     ]
    }
   ],
   "source": [
    "tool_response = tool_map[tool_1_name].invoke(tool_1_args)\n",
    "tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_1_id)\n",
    "\n",
    "print(tool_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `tool_map`, passing in the tool name and parameters to get a response. Then wrap that response in a `ToolMessage` object from LangChain along with the tool call ID. This action allows the model and LangChain to better process tool responses and overall conversation between user and model and tool. Feel free to uncomment the print statement to see what the `tool_message` looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append(tool_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, append the `tool_message` to the `chat_history` so the model preserves context and sees prior conversation for a better conversing experience. Now the chat history contains a `HumanMessage` (initial user query), an `AIMessage` (the response from the model), and a `ToolMessage` (the output of the tool).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a final answer from chat history\n",
    "\n",
    "As a final step, pass the entire `chat_history` into the LLM one more time to get a final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "3 + 2 equals 5.\n"
     ]
    }
   ],
   "source": [
    "answer = llm_with_tools.invoke(chat_history)\n",
    "print(type(answer))\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the `answer.content` (content field of the `AIMessage` object) gives the final result of the LLM for the user query. You have finished a complete interaction between the user and model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Agent\n",
    "\n",
    "You can wrap all the prior functionality in a unified Agent class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCallingAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm_with_tools = llm.bind_tools(tools)\n",
    "        self.tool_map = tool_map\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        # Step 1: Initial user message\n",
    "        chat_history = [HumanMessage(content=query)]\n",
    "\n",
    "        # Step 2: LLM chooses tool\n",
    "        response = self.llm_with_tools.invoke(chat_history)\n",
    "        if not response.tool_calls:\n",
    "            return response.contet # Direct response, no tool needed\n",
    "        # Step 3: Handle first tool call\n",
    "        tool_call = response.tool_calls[0]\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_call_id = tool_call[\"id\"]\n",
    "\n",
    "        # Step 4: Call tool manually\n",
    "        tool_result = self.tool_map[tool_name].invoke(tool_args)\n",
    "\n",
    "        # Step 5: Send result back to LLM\n",
    "        tool_message = ToolMessage(content=str(tool_result), tool_call_id=tool_call_id)\n",
    "        chat_history.extend([response, tool_message])\n",
    "\n",
    "        # Step 6: Final LLM result\n",
    "        final_response = self.llm_with_tools.invoke(chat_history)\n",
    "        return final_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent does the exact same process as above except the interaction with the model handling is all contained within the `run()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One plus two equals three.\n",
      "The result of \\( 1 - 2 \\) is \\(-1\\).\n",
      "Three times two equals 6.\n"
     ]
    }
   ],
   "source": [
    "my_agent = ToolCallingAgent(llm)\n",
    "\n",
    "print(my_agent.run(\"one plus 2\"))\n",
    "\n",
    "print(my_agent.run(\"one - 2\"))\n",
    "\n",
    "print(my_agent.run(\"three times two\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are three examples of the agent in use. These agents are dynamic and can handle many different types and formats of data as input. This capability is a major benefit of AI agents as the input data doesn't need to be normalized or formatted a certain way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've now completed this short introduction to building interactive tool calling agents. Now you can:\n",
    "- Structure user interactions and setup chat models for real-time, context-aware conversations\n",
    "- Extracte tool names and arguments to precisely match user intent\n",
    "- Parse complex tool instructions, including handling multiple tool calls\n",
    "- Build and refine an agent class to automate the entire tool-calling process\n",
    "- Dempnstrate how these components work together to transform LLMs from passive responders to intelligent agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create a new tool\n",
    "\n",
    "Use the example tool format provided in the notebook to create a new tool named `calculate_tip` that takes a `total_bill and tip_percent`, and returns the tip amount. </br>\n",
    "Define and invoke the tool with sample inputs like `total_bill=120`, `tip_percent=15`. </br>\n",
    "Create a `tool_map` with the `calculate_tip` tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 1\n",
    "\n",
    "@tool\n",
    "def calculate_tip(total_bill: int, tip_percent: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the tip to be given based on the total of the bill and tip percentage.\n",
    "    \n",
    "    Args:\n",
    "        total_bill (int): The total amount to be paid for a service\n",
    "        tip_percent (int): The percentage of tip to be given from the total bill\n",
    "\n",
    "    Return:\n",
    "        int: amount to be given as percentage of total bill\n",
    "    \"\"\"\n",
    "    return total_bill * tip_percent/100\n",
    "\n",
    "\n",
    "inputs = {\n",
    "    \"total_bill\": 120,\n",
    "    \"tip_percent\": 15\n",
    "}\n",
    "calculate_tip.invoke(inputs)\n",
    "\n",
    "\n",
    "tool_map = {\n",
    "    \"calculate_tip\": calculate_tip\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def calculate_tip(total_bill: int, tip_percent: int) -> int:\n",
    "    \"\"\"Calculate tip\"\"\"\n",
    "    return total_bill * tip_percent * 0.01\n",
    "\n",
    "inputs = {\n",
    "    \"total_bill\": 120,\n",
    "    \"tip_percent\": 15\n",
    "}\n",
    "calculate_tip.invoke(inputs)\n",
    "\n",
    "\n",
    "tool_map = {\n",
    "    \"calculate_tip\": calculate_tip\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Tool calling with an LLM\n",
    "\n",
    "Simulate a user query like \"How much should I tip on $60 at 20%?\". </br>\n",
    "Bind the tool to the predefined `llm` and prompt the LLM with the query above. Then parse the LLM response for the tool calling details and invoke the tool accordingly. Finally, take the entire chat history and prompt the LLM for a final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 2\n",
    "query = \"How much should I tip on $60 at 20%?\"\n",
    "\n",
    "llm_with_tool = llm.bind_tools([calculate_tip])\n",
    "chat_history = [HumanMessage(content=query)]\n",
    "\n",
    "response = llm_with_tool.invoke(chat_history)\n",
    "\n",
    "tool_calls = response.tool_calls\n",
    "tool_name = tool_calls[0][\"name\"]\n",
    "tool_args = tool_calls[0][\"args\"]\n",
    "tool_call_id = tool_calls[0][\"id\"]\n",
    "\n",
    "tool_response = tool_map[tool_name].invoke(tool_args)\n",
    "tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_id)\n",
    "\n",
    "chat_history.extend([response, tool_message])\n",
    "\n",
    "result = llm_with_tool.invoke(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "llm_with_tool = llm.bind_tools([calculate_tip])\n",
    "chat_history = [HumanMessage(content=query)]\n",
    "\n",
    "response = llm_with_tool.invoke(chat_history)\n",
    "\n",
    "tool_calls = response.tool_calls\n",
    "tool_name = tool_calls[0][\"name\"]\n",
    "tool_args = tool_calls[0][\"args\"]\n",
    "tool_call_id = tool_calls[0][\"id\"]\n",
    "\n",
    "tool_response = tool_map[tool_name].invoke(tool_args)\n",
    "tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_id)\n",
    "\n",
    "chat_history.extend([response, tool_message])\n",
    "\n",
    "result = llm_with_tool.invoke(chat_history)\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Create a tip calculating agent\n",
    "\n",
    "Create an agent to automate the entire process you previously completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You should tip $12 on a $60 bill at 20%.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Exercise 3\n",
    "class TipAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm_with_tool = llm.bind_tools([calculate_tip])\n",
    "        self.tool_map = tool_map\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        chat_history = [HumanMessage(content=query)]\n",
    "        response = llm_with_tool.invoke(chat_history)\n",
    "\n",
    "        tool_calls = response.tool_calls\n",
    "        tool_name = tool_calls[0][\"name\"]\n",
    "        tool_args = tool_calls[0][\"args\"]\n",
    "        tool_call_id = tool_calls[0][\"id\"]\n",
    "        \n",
    "        tool_response = tool_map[tool_name].invoke(tool_args)\n",
    "        tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_id)\n",
    "        \n",
    "        chat_history.extend([response, tool_message])\n",
    "        \n",
    "        return llm_with_tool.invoke(chat_history).content\n",
    "\n",
    "agent = TipAgent(llm)\n",
    "agent.run(\"How much should I tip on $60 at 20%?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "class TipAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm_with_tool = llm.bind_tools([calculate_tip])\n",
    "        self.tool_map = tool_map\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        chat_history = [HumanMessage(content=query)]\n",
    "        response = llm_with_tool.invoke(chat_history)\n",
    "\n",
    "        tool_calls = response.tool_calls\n",
    "        tool_name = tool_calls[0][\"name\"]\n",
    "        tool_args = tool_calls[0][\"args\"]\n",
    "        tool_call_id = tool_calls[0][\"id\"]\n",
    "        \n",
    "        tool_response = tool_map[tool_name].invoke(tool_args)\n",
    "        tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_id)\n",
    "        \n",
    "        chat_history.extend([response, tool_message])\n",
    "        \n",
    "        return llm_with_tool.invoke(chat_history).content\n",
    "\n",
    "agent = TipAgent(llm)\n",
    "agent.run(\"How much should I tip on $60 at 20%?\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joshua Zhou](https://author.skills.network/instructors/joshua_zhou)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kunal Makwana](https://author.skills.network/instructors/kunal_makwana)</br>\n",
    "[Karan Goswami](https://author.skills.network/instructors/karan_goswami)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Changelog\n",
    "\n",
    "| Date | Version | Changed by | Change Description |\n",
    "|------|--------|--------|---------|\n",
    "| 2024-06-06 | 0.1 |  P. Kravitz | ID review and edit. No code edits.Updated the copyright statement. Change log added. Instructional edits only for IBM style. Second person, accessibility, and other minor grammar edits.| -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "3ed25488039fa4c590a60c6be98ebd97e5fa32f9525446191ec94fcf70e9fc62"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
