{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Build a Tool Calling Agent**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **1** hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll explore the powerful capabilities of tool calling in large language models (LLMs) to build advanced AI agents that can interact with external systems. You'll learn how to create custom tools that enable an LLM to perform specific actions, from extracting video IDs to fetching YouTube transcripts and metadata. Through hands-on examples, you'll first implement manual tool calling to understand the underlying mechanics, then build a flexible YouTube interaction system that can search videos, extract transcripts, fetch trending content, and generate summaries. By the end of this lab, you'll understand how to construct both fixed-sequence and recursive tool-calling chains, allowing your AI assistants to dynamically decide which tools to use and when to use them, creating truly intelligent agents that can reason about and interact with the world around them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "   <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "   <li>\n",
    "       <a href=\"#Setup\">Setup</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "           <li><a href=\"#Importing-Required-Libraries\">Importing required libraries</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Tools\">Tools</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Defining-video-ID-extraction-tool\">Defining video ID extraction tool</a></li>\n",
    "           <li><a href=\"#Tool-list\">Tool list</a></li>\n",
    "           <li><a href=\"#Defining-transcript-fetching-tool\">Defining transcript fetching tool</a></li>\n",
    "           <li><a href=\"#Defining-YouTube-search-tool\">Defining YouTube search tool</a></li>\n",
    "           <li><a href=\"#Defining-metadata-extraction-tool\">Defining metadata extraction tool</a></li>\n",
    "           <li><a href=\"#Defining-trending-videos-tool\">Defining trending videos tool</a></li>\n",
    "           <li><a href=\"#Defining-thumbnail-retrieval-tool\">Defining thumbnail retrieval tool</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Binding-tools\">Binding tools</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#How-the-LLM-calls-a-tool\">How the LLM calls a tool</a></li>\n",
    "           <li><a href=\"#LangChain-tool-binding-process\">LangChain tool binding process</a></li>\n",
    "           <li><a href=\"#Extracting-tool-call-information\">Extracting tool call information</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Automating-the-tool-calling-process\">Automating the tool calling process</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Building-the-summarization-chain\">Building the summarization chain</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Recursive-chain-flow\">Recursive chain flow</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Defining-the-core-processing-logic\">Defining the core processing logic</a></li>\n",
    "           <li><a href=\"#Building-the-complete-universal-chain\">Building the complete universal chain</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "</ol>\n",
    "\n",
    "<li><a href=\"#Exercise\">Exercise</a></li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "- Create custom tools that extend the capabilities of language models\n",
    "- Build both manual and automated tool calling chains\n",
    "- Implement recursive tool calling for dynamic, multi-step operations\n",
    "- Develop AI agents that can interact with YouTube's content programmatically\n",
    "- Apply tool calling techniques to extract, process, and summarize information from external sources\n",
    "- Design flexible workflows that allow LLMs to reason about when and how to use available tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "*   [`pytube`](https://pytube.io/en/latest/) for accessing YouTube videos and their metadata programmatically.\n",
    "*   [`youtube-transcript-api`](https://github.com/jdepoix/youtube-transcript-api) for fetching transcripts from YouTube videos.\n",
    "*   [`langchain`](https://python.langchain.com/docs/get_started/introduction) for building tool-enabled LLM applications.\n",
    "*   [`langchain-community`](https://python.langchain.com/docs/integrations/providers/) for additional LangChain integrations.\n",
    "*   [`langchain-openai`](https://python.langchain.com/docs/integrations/llms/openai) for connecting to OpenAI's language models.\n",
    "*   [`yt-dlp`](https://github.com/yt-dlp/yt-dlp) for enhanced YouTube data extraction capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pytube \n",
    "%pip install youtube-transcript-api==1.1.0\n",
    "%pip install langchain-community==0.3.16\n",
    "%pip install langchain==0.3.23\n",
    "%pip install langchain-openai==0.3.14\n",
    "%pip install yt-dlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended that you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pytube import YouTube\n",
    "from langchain_core.tools import tool\n",
    "from IPython.display import display, JSON\n",
    "import yt_dlp\n",
    "from typing import List, Dict\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import ToolMessage\n",
    "import json\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress pytube errors\n",
    "import logging\n",
    "pytube_logger = logging.getLogger('pytube')\n",
    "pytube_logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress yt-dlp warnings\n",
    "yt_dpl_logger = logging.getLogger('yt_dlp')\n",
    "yt_dpl_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the language model that will power your tool calling capabilities. This code sets up a GPT-4o-mini model using the OpenAI provider through LangChain's interface, which you'll use to process queries and decide which tools to call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Disclaimer\n",
    "This lab uses LLMs provided by OpenAI. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to configure your own API keys. Please note that using your own API keys means that you will incur personal charges.\n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API key. This lab uses the `init_chat_model` function from `langchain`. To use the model you must set the environment variable `OPENAI_API_KEY` to your OpenAI API key. **DO NOT** run the cell below if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# IGNORE IF YOU ARE NOT RUNNING LOCALLY\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour OpenAI API key here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# IGNORE IF YOU ARE NOT RUNNING LOCALLY\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your OpenAI API key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating custom tools with LangChain\n",
    "\n",
    "### Anatomy of a tool\n",
    "\n",
    "Let's provide the basic building blooks a  tool, consider the following tools:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def tool_name(input_param: input_type) -> output_type:\n",
    "   \"\"\"\n",
    "   Clear description of what the tool does.\n",
    "   \n",
    "   Args:\n",
    "       input_param (input_type): Description of this parameter\n",
    "   \n",
    "   Returns:\n",
    "       output_type: Description of what is returned\n",
    "   \"\"\"\n",
    "   # Function implementation\n",
    "   result = process(input_param)\n",
    "   return result\n",
    "```\n",
    "\n",
    "\n",
    "## Key components\n",
    "\n",
    "1. **@tool decorator**\n",
    "   - Registers the function with LangChain\n",
    "   - Creates tool attributes (.name, .description, .func)\n",
    "   - Generates JSON schema for validation\n",
    "   - Transforms regular functions into callable tools\n",
    "\n",
    "2. **Function name**\n",
    "   - Used by LLM to select appropriate tool\n",
    "   - Used as reference in chains and tool mappings\n",
    "   - Appears in tool call logs for debugging\n",
    "   - Should clearly indicate the tool's purpose\n",
    "\n",
    "3. **Type annotations**\n",
    "   - Enable automatic input validation\n",
    "   - Create schema for parameters\n",
    "   - Allow proper serialization of inputs/outputs\n",
    "   - Help LLM understand required input formats\n",
    "\n",
    "4. **Docstring**\n",
    "   - Provides context for the LLM to decide when to use the tool\n",
    "   - Documents parameter requirements\n",
    "   - Explains expected outputs and behavior\n",
    "   - Critical for tool selection by the LLM\n",
    "\n",
    "5. **Implementation**\n",
    "   - Executes the actual operation\n",
    "   - Handles errors appropriately\n",
    "   - Returns properly formatted results\n",
    "   - Should be efficient and robust\n",
    "\n",
    "\n",
    "### Defining video ID extraction tool\n",
    "\n",
    "Now you'll define a function `extract_video_id` by denoting it as a tool that will help you to extract the video ID from a given URL. This is necessary because many YouTube API operations, including transcript extraction, require the video ID rather than the complete URL. The function uses regular expressions to handle different YouTube URL formats (standard, shortened, and embedded) and extract the 11-character video ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def extract_video_id(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the 11-character YouTube video ID from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): A YouTube URL containing a video ID.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted video ID or error message if parsing fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regex pattern to match video IDs\n",
    "    pattern = r'(?:v=|be/|embed/)([a-zA-Z0-9_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else \"Error: Invalid YouTube URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decorator wraps your function, adding those attributes (.name, .description, .func) and registering it with LangChain's tool system. The original function becomes accessible through the .func attribute, but the overall object is an instance of LangChain's tool class, with additional methods like .run() for direct invocation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Testing the video ID extraction tool\n",
    "\n",
    "\n",
    "Now you'll be testing your `extract_video_id` tool to verify that it's correctly registered with LangChain. These print statements will show you:\n",
    "1. The tool's name (as it will be referenced by the LLM)\n",
    "2. The tool's description (which helps the LLM understand when to use this tool)\n",
    "3. The actual function reference that will be called\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_video_id\n",
      "----------------------------\n",
      "Extracts the 11-character YouTube video ID from a URL.\n",
      "\n",
      "Args:\n",
      "    url (str): A YouTube URL containing a video ID.\n",
      "\n",
      "Returns:\n",
      "    str: Extracted video ID or error message if parsing fails.\n",
      "----------------------------\n",
      "<function extract_video_id at 0x7ee45d5e9da0>\n"
     ]
    }
   ],
   "source": [
    "print(extract_video_id.name)\n",
    "print(\"----------------------------\")\n",
    "print(extract_video_id.description)\n",
    "print(\"----------------------------\")\n",
    "print(extract_video_id.func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing tool execution\n",
    "\n",
    "Here, you're testing the actual execution of your `extract_video_id` tool with a real YouTube URL. You can call the tool using the `.run()` method, which is a convenient way to execute the tool directly and see its output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hfIUstzHs9A'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_video_id.run(\"https://www.youtube.com/watch?v=hfIUstzHs9A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='extract_video_id', description='Extracts the 11-character YouTube video ID from a URL.\\n\\nArgs:\\n    url (str): A YouTube URL containing a video ID.\\n\\nReturns:\\n    str: Extracted video ID or error message if parsing fails.', args_schema=<class 'langchain_core.utils.pydantic.extract_video_id'>, func=<function extract_video_id at 0x7ee45d5e9da0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_video_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows that your function has been transformed into a `StructuredTool` object by LangChain. It displays the tool's name ('extract_video_id'), its description (our docstring), a Pydantic schema for input validation, and a reference to your original function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool list \n",
    "Multiple tools will be created to enhance the LLM's capabilities. For organization, create a list called tools, which is a standard Python list that contains tool objects created with the @tool decorator. This list doesn't execute functions or determine call order - it simply collects tool objects in one place so they can be efficiently passed to the language model via llm.bind_tools(tools). This approach allows the LLM to access all available tools without requiring them to be individually registered.\n",
    "\n",
    "Adding the ```extract_video_id``` tool to your tools list, which you can later provide to the LLM so it can use this functionality when needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "tools.append(extract_video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have understood the basic structure, let's define the rest of the tools you'll need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining transcript fetching tool\n",
    "\n",
    "Now you're going to create another tool that fetches the transcript from a YouTube video. This tool uses the `YouTubeTranscriptApi` library to retrieve the captions or subtitles from a video. You'll be taking the video ID (which can be extracted using your previous tool) and an optional language parameter. The function attempts to get the transcript and joins all text segments into a continuous string, or returns an error message if the transcript can't be retrieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_transcript(video_id: str, language: str = \"en\") -> str:\n",
    "    \"\"\"\n",
    "    Fetches the transcript of a YouTube video.\n",
    "    \n",
    "    Args:\n",
    "        video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n",
    "        language (str): Language code for the transcript (e.g., \"en\", \"es\").\n",
    "    \n",
    "    Returns:\n",
    "        str: The transcript text or an error message.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        ytt_api = YouTubeTranscriptApi()\n",
    "        transcript = ytt_api.fetch(video_id, languages=[language])\n",
    "        return \" \".join([snippet.text for snippet in transcript.snippets])\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the fetch_transcript tool by directly calling it with the .run() method on a specific video ID. This will attempt to retrieve the transcript for the video with ID \"hfIUstzHs9A\" in the default English language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over the past couple of months, large language models, or LLMs, such as chatGPT, have taken the world by storm. Whether it\\'s writing poetry or helping plan your upcoming vacation, we are seeing a step change in the performance of AI and its potential to drive enterprise value. My name is Kate Soule. I\\'m a senior manager of business strategy at IBM Research, and today I\\'m going to give a brief overview of this new field of AI that\\'s emerging and how it can be used in a business setting to drive value. Now, large language models are actually a part of a different class of models called foundation models. Now, the term \"foundation models\" was actually first coined by a team from Stanford when they saw that the field of AI was converging to a new paradigm. Where before AI applications were being built by training, maybe a library of different AI models, where each AI model was trained on very task-specific data to perform very specific task. They predicted that we were going to start moving to a new paradigm, where we would have a foundational capability, or a foundation model, that would drive all of these same use cases and applications. So the same exact applications that we were envisioning before with conventional AI, and the same model could drive any number of additional applications. The point is that this model could be transferred to any number of tasks. What gives this model the super power to be able to transfer to multiple different tasks and perform multiple different functions is that it\\'s been trained on a huge amount, in an unsupervised manner, on unstructured data. And what that means, in the language domain, is basically I\\'ll feed a bunch of sentences-- and I\\'m talking terabytes of data here --to train this model. And the start of my sentence might be \"no use crying over spilled\" and the end of my sentence might be \"milk\". And I\\'m trying to get my model to predict the last word of the sentence based off of the words that it saw before. And it\\'s this generative capability of the model-- predicting and generating the next word --based off of previous words that it\\'s seen beforehand, that is why that foundation models are actually a part of the field of AI called generative AI because we\\'re generating something new in this case, the next word in a sentence. And even though these models are trained to perform, at its core, a generation past, predicting the next word in the sentence, we actually can take these models, and if you introduce a small amount of labeled data to the equation, you can tune them to perform traditional NLP tasks-- things like classification, or named-entity recognition --things that you don\\'t normally associate as being a generative-based model or capability. And this process is called tuning. Where you can tune your foundation model by introducing a small amount of data, you update the parameters of your model and now perform a very specific natural language task. If you don\\'t have data, or have only very few data points, you can still take these foundation models and they actually work very well in low-labeled data domains. And in a process called prompting or prompt engineering, you can apply these models for some of those same exact tasks. So an example of prompting a model to perform a classification task might be you could give a model a sentence and then ask it a question: Does this sentence have a positive sentiment or negative sentiment? The model\\'s going to try and finish generating words in that sentence, and the next natural word in that sentence would be the answer to your classification problem, which would respond either positive or negative, depending on where it estimated the sentiment of the sentence would be. And these models work surprisingly well when applied to these new settings and domains. Now, this is a lot of where the advantages of foundation models come into play. So if we talk about the advantages, the chief advantage is the performance. These models have seen so much data. Again, data with a capital D-- terabytes of data --that by the time that they\\'re applied to small tasks, they can drastically outperform a model that was only trained on just a few data points. The second advantage of these models are the productivity gains. So just like I said earlier, through prompting or tuning, you need far less label data to get to task-specific model than if you had to start from scratch because your model is taking advantage of all the unlabeled data that it saw in its pre-training when we created this generative task. With these advantages, there are also some disadvantages that are important to keep in mind. And the first of those is the compute cost. So that penalty for having this model see so much data is that they\\'re very expensive to train, making it difficult for smaller enterprises to train a foundation model on their own. They\\'re also expensive-- by the time they get to a huge size, a couple billion parameters --they\\'re also very expensive to run inference. You might require multiple GPUs at a time just to host these models and run inference, making them a more costly method than traditional approaches. The second disadvantage of these models is on the trustworthiness side. So just like data is a huge advantage for these models, they\\'ve seen so much unstructured data, it also comes at a cost, especially in the domain like language. A lot of these models are trained basically off of language data that\\'s been scraped from the Internet. And there\\'s so much data that these models have been trained on. Even if you had a whole team of human annotators, you wouldn\\'t be able to go through and actually vet every single data point to make sure that it wasn\\'t biased and didn\\'t contain hate speech or other toxic information. And that\\'s just assuming you actually know what the data is. Often we don\\'t even know-- for a lot of these open source models that have been posted --what the exact datasets are that these models have been trained on leading to trustworthiness issues. So IBM recognizes the huge potential of these technologies. But my partners in IBM Research are working on multiple different innovations to try and improve also the efficiency of these models and the trustworthiness and reliability of these models to make them more relevant in a business setting. All of these examples that I\\'ve talked through so far have just been on the language side. But the reality is, there are a lot of other domains that foundation models can be applied towards. Famously, we\\'ve seen foundation models for vision --looking at models such as DALL-E 2, which takes text data, and that\\'s then used to generate a custom image. We\\'ve seen models for code with products like Copilot that can help complete code as it\\'s being authored. And IBM\\'s innovating across all of these domains. So whether it\\'s language models that we\\'re building into products like Watson Assistant and Watson Discovery, vision models that we\\'re building into products like Maximo Visual Inspection, or Ansible code models that we\\'re building with our partners at Red Hat under Project Wisdom. We\\'re innovating across all of these domains and more. We\\'re working on chemistry. So, for example, we just published and released molformer, which is a foundation model to promote molecule discovery or different targeted therapeutics. And we\\'re working on models for climate change, building Earth Science Foundation models using geospatial data to improve climate research. I hope you found this video both informative and helpful. If you\\'re interested in learning more, particularly how IBM is working to improve some of these disadvantages, making foundation models more trustworthy and more efficient, please take a look at the links below. Thank you.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_transcript.run(\"hfIUstzHs9A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Adding the `fetch_transcript` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(fetch_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining YouTube search tool\n",
    "\n",
    "Now let's create a search tool that allows finding videos on YouTube based on a query string. This tool uses the `Search` class from the PyTube library to perform searches on YouTube. When given a search term, it returns a list of matching videos with each video represented as a dictionary containing the title, video ID, and a shortened URL. This tool will be helpful for discovering relevant videos when you don't already have a specific URL in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import Search\n",
    "from langchain.tools import tool\n",
    "from typing import List, Dict\n",
    "\n",
    "@tool\n",
    "def search_youtube(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Search YouTube for videos matching the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search term to look for on YouTube\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing video titles and IDs in format:\n",
    "        [{'title': 'Video Title', 'video_id': 'abc123'}, ...]\n",
    "        Returns error message if search fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = Search(query)\n",
    "        return [\n",
    "            {\n",
    "                \"title\": yt.title,\n",
    "                \"video_id\": yt.video_id,\n",
    "                \"url\": f\"https://youtu.be/{yt.video_id}\"\n",
    "            }\n",
    "            for yt in s.results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `search_youtube` tool by calling it with the `.run()` method and the search query \"Generative AI.\" This will return a list of YouTube videos related to generative AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "title": "Generative AI in a Nutshell - how to survive and thrive in the age of AI",
        "url": "https://youtu.be/2IK3DFHRFfw",
        "video_id": "2IK3DFHRFfw"
       },
       {
        "title": "What is generative AI and how does it work? – The Turing Lectures with Mirella Lapata",
        "url": "https://youtu.be/_6R7Ym6Vy_I",
        "video_id": "_6R7Ym6Vy_I"
       },
       {
        "title": "AI-900 - Learning About Generative AI",
        "url": "https://youtu.be/Ch6KE7KxHGM",
        "video_id": "Ch6KE7KxHGM"
       },
       {
        "title": "AI, Machine Learning, Deep Learning and Generative AI Explained",
        "url": "https://youtu.be/qYNweeDHiyU",
        "video_id": "qYNweeDHiyU"
       },
       {
        "title": "Generative AI Explained In 5 Minutes | What Is GenAI? | Introduction To Generative AI | Simplilearn",
        "url": "https://youtu.be/NRmAXDWJVnU",
        "video_id": "NRmAXDWJVnU"
       },
       {
        "title": "Generative AI explained in 2 minutes",
        "url": "https://youtu.be/rwF-X5STYks",
        "video_id": "rwF-X5STYks"
       },
       {
        "title": "Introduction to Generative AI",
        "url": "https://youtu.be/G2fqAlgmoPo",
        "video_id": "G2fqAlgmoPo"
       },
       {
        "title": "Generative AI's Greatest Flaw - Computerphile",
        "url": "https://youtu.be/rAEqP9VEhe8",
        "video_id": "rAEqP9VEhe8"
       },
       {
        "title": "🔵 Build Your Own AI Agent-Based Solution with the Generative AI Hub (Java & SpringAI version)",
        "url": "https://youtu.be/DRQfo5X6MOc",
        "video_id": "DRQfo5X6MOc"
       },
       {
        "title": "What's the future for generative AI? - The Turing Lectures with Mike Wooldridge",
        "url": "https://youtu.be/b76gsOSkHB4",
        "video_id": "b76gsOSkHB4"
       },
       {
        "title": "Generative AI Full Course – Gemini Pro, OpenAI, Llama, Langchain, Pinecone, Vector Databases & More",
        "url": "https://youtu.be/mEsleV16qdo",
        "video_id": "mEsleV16qdo"
       },
       {
        "title": "Watch this before using generative AI",
        "url": "https://youtu.be/unPKJJjQP0A",
        "video_id": "unPKJJjQP0A"
       },
       {
        "title": "Generative AI is not the panacea we’ve been promised | Eric Siegel for Big Think+",
        "url": "https://youtu.be/B2zCWJBnfuE",
        "video_id": "B2zCWJBnfuE"
       },
       {
        "title": "Generative vs Agentic AI: Shaping the Future of AI Collaboration",
        "url": "https://youtu.be/EDb37y_MhRw",
        "video_id": "EDb37y_MhRw"
       },
       {
        "title": "What is Generative AI?",
        "url": "https://youtu.be/pWNAtUwnBS8",
        "video_id": "pWNAtUwnBS8"
       },
       {
        "title": "Integrating Generative AI Into Business Strategy: Dr. George Westerman",
        "url": "https://youtu.be/9RvWcXVaAng",
        "video_id": "9RvWcXVaAng"
       },
       {
        "title": "Generative AI for Developers – Comprehensive Course",
        "url": "https://youtu.be/F0GQ0l2NfHA",
        "video_id": "F0GQ0l2NfHA"
       },
       {
        "title": "Variational Autoencoders | Generative AI Animated",
        "url": "https://youtu.be/qJeaCHQ1k2w",
        "video_id": "qJeaCHQ1k2w"
       },
       {
        "title": "The Turing Lectures: The future of generative AI",
        "url": "https://youtu.be/2kSl0xkq2lM",
        "video_id": "2kSl0xkq2lM"
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_out=search_youtube.run(\"Generative AI\")\n",
    "display(JSON(search_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending the `search_youtube` tool to tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(search_youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining metadata extraction tool\n",
    "\n",
    "Now you'll create a tool that extracts detailed metadata from a YouTube video using the `yt-dlp` library. This tool takes a YouTube URL and returns comprehensive information about the video, including its title, view count, duration, channel name, like count, comment count, and any chapter markers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_full_metadata(url: str) -> dict:\n",
    "    \"\"\"Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.\"\"\"\n",
    "    with yt_dlp.YoutubeDL({'quiet': True, 'logger': yt_dpl_logger}) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "        return {\n",
    "            'title': info.get('title'),\n",
    "            'views': info.get('view_count'),\n",
    "            'duration': info.get('duration'),\n",
    "            'channel': info.get('uploader'),\n",
    "            'likes': info.get('like_count'),\n",
    "            'comments': info.get('comment_count'),\n",
    "            'chapters': info.get('chapters', [])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_full_metadata` tool by running it on a specific YouTube video URL. This will extract comprehensive information about the video with ID \"qWHaMrR5WHQ\" without downloading the actual video content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "channel": "Praveen Reddy Learnings",
       "chapters": null,
       "comments": 5,
       "duration": 1048,
       "likes": 24,
       "title": "Explained how to use Tools (tool calling) in LangChain",
       "views": 1431
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_data=get_full_metadata.run(\"https://youtu.be/qWHaMrR5WHQ\")\n",
    "display(JSON(meta_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the `get_full_metadata` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_full_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining trending videos tool\n",
    "\n",
    "Now, you'll create a tool to fetch the currently trending videos on YouTube for a specific region. This tool uses `yt-dlp` to access YouTube's trending feed based on a provided country code (like \"US\" for the United States or \"IN\" for India). It collects important information about each trending video, including the title, video ID, URL, channel name, duration, and view count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "@tool\n",
    "def get_trending_videos(region_code: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches currently trending YouTube videos for a specific region.\n",
    "    \n",
    "    Args:\n",
    "        region_code (str): 2-letter country code (e.g., \"US\", \"IN\", \"GB\")\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with video details: title, video_id, channel, view_count, duration\n",
    "    \"\"\"\n",
    "    ydl_opts = {\n",
    "        'geo_bypass_country': region_code.upper(),\n",
    "        'extract_flat': True,\n",
    "        'quiet': True,\n",
    "        'force_generic_extractor': True,\n",
    "        'logger': yt_dpl_logger\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(\n",
    "                'https://www.youtube.com/feed/trending',\n",
    "                download=False\n",
    "            )\n",
    "            \n",
    "            trending_videos = []\n",
    "            for entry in info['entries']:\n",
    "                video_data = {\n",
    "                    'title': entry.get('title', 'N/A'),\n",
    "                    'video_id': entry.get('id', 'N/A'),\n",
    "                    'url': entry.get('url', 'N/A'),\n",
    "                    'channel': entry.get('uploader', 'N/A'),\n",
    "                    'duration': entry.get('duration', 0),\n",
    "                    'view_count': entry.get('view_count', 0)\n",
    "                }\n",
    "                trending_videos.append(video_data)\n",
    "                \n",
    "            return trending_videos[:25]  # Return top 25 trending videos\n",
    "            \n",
    "    except Exception as e:\n",
    "        return [{'error': f\"Failed to fetch trending videos: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_trending_videos` tool by running it with the region code `\"US\"` to fetch trending videos from the United States. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube:tab] trending: The channel/playlist does not exist and the URL redirected to youtube.com home page\n"
     ]
    },
    {
     "data": {
      "application/json": [
       {
        "error": "Failed to fetch trending videos: ERROR: [youtube:tab] trending: The channel/playlist does not exist and the URL redirected to youtube.com home page"
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trending_videos=get_trending_videos.run(\"US\")\n",
    "# Display as formatted JSON\n",
    "display(JSON(trending_videos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the `get_trending_videos` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_trending_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining thumbnail retrieval tool\n",
    "\n",
    "Now you'll create a tool to extract all available thumbnail images for a YouTube video. This tool uses `yt-dlp` to retrieve information about the various thumbnail images that YouTube generates for videos at different resolutions. For each thumbnail, collect its URL, width, height, and formatted resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_thumbnails(url: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get available thumbnails for a YouTube video using its URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): YouTube video URL (any format)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with thumbnail URLs and resolutions in YouTube's native order\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL({'quiet': True, 'logger': yt_dpl_logger}) as ydl:\n",
    "            info = ydl.extract_info(url, download=False)\n",
    "            \n",
    "            thumbnails = []\n",
    "            for t in info.get('thumbnails', []):\n",
    "                if 'url' in t:\n",
    "                    thumbnails.append({\n",
    "                        \"url\": t['url'],\n",
    "                        \"width\": t.get('width'),\n",
    "                        \"height\": t.get('height'),\n",
    "                        \"resolution\": f\"{t.get('width', '')}x{t.get('height', '')}\".strip('x')\n",
    "                    })\n",
    "            \n",
    "            return thumbnails\n",
    "\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Failed to get thumbnails: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_thumbnails` tool by running it on a specific YouTube video URL. This will extract information about all available thumbnail images for the video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mq3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mq2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mq1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sd3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sd2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sd1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/default.jpg",
        "width": null
       },
       {
        "height": 90,
        "resolution": "120x90",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/default.webp",
        "width": 120
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mqdefault.jpg",
        "width": null
       },
       {
        "height": 180,
        "resolution": "320x180",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mqdefault.webp",
        "width": 320
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/0.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/0.webp",
        "width": null
       },
       {
        "height": 94,
        "resolution": "168x94",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLDK33nNmNAuVa1ibN7uTkq5okdwfw",
        "width": 168
       },
       {
        "height": 94,
        "resolution": "168x94",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEiCKgBEF5IWvKriqkDFQgBFQAAAAAYASUAAMhCPQCAokN4AQ==&rs=AOn4CLBQiFoh8Nia1_XrFTBE3-g4hp61xQ",
        "width": 168
       },
       {
        "height": 110,
        "resolution": "196x110",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEbCMQBEG5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLBa9uAOnYdtfae6yOMFyYfUFEvjkA",
        "width": 196
       },
       {
        "height": 110,
        "resolution": "196x110",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEiCMQBEG5IWvKriqkDFQgBFQAAAAAYASUAAMhCPQCAokN4AQ==&rs=AOn4CLDw2g-OpL84ZvPbhVdQ9S_Unp3RMg",
        "width": 196
       },
       {
        "height": 138,
        "resolution": "246x138",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEcCPYBEIoBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLDjZI5FIYKE2PlZhm9PQTk0eKeDAw",
        "width": 246
       },
       {
        "height": 138,
        "resolution": "246x138",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEjCPYBEIoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLCJk1-Lqfb8XHsFyiBDiCN0opMcDA",
        "width": 246
       },
       {
        "height": 188,
        "resolution": "336x188",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEcCNACELwBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLBTNOqMuqjK5oJ-_mtGsUJOh-sVkA",
        "width": 336
       },
       {
        "height": 188,
        "resolution": "336x188",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEjCNACELwBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLAlaeOG0AijM70t7qsBkkaEvRb3FQ",
        "width": 336
       },
       {
        "height": 360,
        "resolution": "480x360",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg",
        "width": 480
       },
       {
        "height": 360,
        "resolution": "480x360",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hqdefault.webp",
        "width": 480
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sddefault.jpg",
        "width": null
       },
       {
        "height": 480,
        "resolution": "640x480",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sddefault.webp",
        "width": 640
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq720.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq720.webp",
        "width": null
       },
       {
        "height": 1080,
        "resolution": "1920x1080",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/maxresdefault.jpg",
        "width": 1920
       },
       {
        "height": 1080,
        "resolution": "1920x1080",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/maxresdefault.webp",
        "width": 1920
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thumbnails=get_thumbnails.run(\"https://www.youtube.com/watch?v=qWHaMrR5WHQ\")\n",
    "\n",
    "display(JSON(thumbnails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the `get_thumbnails` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_thumbnails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Binding tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, you'll bind your collection of tools to the language model. It enables the LLM to access and use your custom YouTube tools during conversations. By binding the tools, you're giving the model the ability to call these functions when it determines they're needed to fulfill a user request, making the LLM aware of your tools' capabilities and how to use them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```bind_tools()``` function passes all this information to the language model. It converts each tool's attributes (name, description, parameters schema) into a standardized format that the LLM can understand and use to determine when and how to call specific tools based on user requests. Similar to the following code where the schema for each tool is stored:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "description": "Extracts the 11-character YouTube video ID from a URL.\n\nArgs:\n    url (str): A YouTube URL containing a video ID.\n\nReturns:\n    str: Extracted video ID or error message if parsing fails.",
       "name": "extract_video_id",
       "parameters": {
        "description": "Extracts the 11-character YouTube video ID from a URL.\n\nArgs:\n    url (str): A YouTube URL containing a video ID.\n\nReturns:\n    str: Extracted video ID or error message if parsing fails.",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "extract_video_id",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Fetches the transcript of a YouTube video.\n\nArgs:\n    video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n    language (str): Language code for the transcript (e.g., \"en\", \"es\").\n\nReturns:\n    str: The transcript text or an error message.",
       "name": "fetch_transcript",
       "parameters": {
        "description": "Fetches the transcript of a YouTube video.\n\nArgs:\n    video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n    language (str): Language code for the transcript (e.g., \"en\", \"es\").\n\nReturns:\n    str: The transcript text or an error message.",
        "properties": {
         "language": {
          "default": "en",
          "title": "Language",
          "type": "string"
         },
         "video_id": {
          "title": "Video Id",
          "type": "string"
         }
        },
        "required": [
         "video_id"
        ],
        "title": "fetch_transcript",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Search YouTube for videos matching the query.\n\nArgs:\n    query (str): The search term to look for on YouTube\n\nReturns:\n    List of dictionaries containing video titles and IDs in format:\n    [{'title': 'Video Title', 'video_id': 'abc123'}, ...]\n    Returns error message if search fails",
       "name": "search_youtube",
       "parameters": {
        "description": "Search YouTube for videos matching the query.\n\nArgs:\n    query (str): The search term to look for on YouTube\n\nReturns:\n    List of dictionaries containing video titles and IDs in format:\n    [{'title': 'Video Title', 'video_id': 'abc123'}, ...]\n    Returns error message if search fails",
        "properties": {
         "query": {
          "title": "Query",
          "type": "string"
         }
        },
        "required": [
         "query"
        ],
        "title": "search_youtube",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.",
       "name": "get_full_metadata",
       "parameters": {
        "description": "Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "get_full_metadata",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Fetches currently trending YouTube videos for a specific region.\n\nArgs:\n    region_code (str): 2-letter country code (e.g., \"US\", \"IN\", \"GB\")\n\nReturns:\n    List of dictionaries with video details: title, video_id, channel, view_count, duration",
       "name": "get_trending_videos",
       "parameters": {
        "description": "Fetches currently trending YouTube videos for a specific region.\n\nArgs:\n    region_code (str): 2-letter country code (e.g., \"US\", \"IN\", \"GB\")\n\nReturns:\n    List of dictionaries with video details: title, video_id, channel, view_count, duration",
        "properties": {
         "region_code": {
          "title": "Region Code",
          "type": "string"
         }
        },
        "required": [
         "region_code"
        ],
        "title": "get_trending_videos",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Get available thumbnails for a YouTube video using its URL.\n\nArgs:\n    url (str): YouTube video URL (any format)\n\nReturns:\n    List of dictionaries with thumbnail URLs and resolutions in YouTube's native order",
       "name": "get_thumbnails",
       "parameters": {
        "description": "Get available thumbnails for a YouTube video using its URL.\n\nArgs:\n    url (str): YouTube video URL (any format)\n\nReturns:\n    List of dictionaries with thumbnail URLs and resolutions in YouTube's native order",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "get_thumbnails",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tool in tools:\n",
    "    schema = {\n",
    "   \"name\": tool.name,\n",
    "   \"description\": tool.description,\n",
    "   \"parameters\": tool.args_schema.schema() if tool.args_schema else {},\n",
    "   \"return\": tool.return_type if hasattr(tool, \"return_type\") else None}\n",
    "    display(JSON(schema))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the LLM calls a tool\n",
    "\n",
    "Now, define a sample user query that asks for a summary of a specific YouTube video. This query will be used to demonstrate how your LLM can understand a natural language request and use the appropriate tools you've provided to fulfill it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\n"
     ]
    }
   ],
   "source": [
    "query = \"I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating a message object to represent your user query. You'll be wrapping the query string in a HumanMessage object, which is the standard way to format user inputs in LangChain. It represents a human message as a person is expected to initiate the interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content = query)]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain tool binding process\n",
    "\n",
    "This step involves sending your message to the LLM and storing its response. Here you'll invoke the language model with your user query about summarizing a YouTube video. The response will contain both text content and potentially tool calls that the model decides to make. ``response_1`` contains the LLM's response to the user message, including any tool calls it decides to make. The response object contains the content of the LLM's reply plus structured information about which tools it wants to call and with what parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_JuiezhAMJKjs82avlkRJIRDV', 'function': {'arguments': '{\"url\":\"https://www.youtube.com/watch?v=T-D1OfcDW1M\"}', 'name': 'extract_video_id'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 457, 'total_tokens': 486, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CUY6xO2cBQC4xBvH4Bls3dX2isToL', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--9cc054cc-fa44-4884-8ffd-e8162201582e-0', tool_calls=[{'name': 'extract_video_id', 'args': {'url': 'https://www.youtube.com/watch?v=T-D1OfcDW1M'}, 'id': 'call_JuiezhAMJKjs82avlkRJIRDV', 'type': 'tool_call'}], usage_metadata={'input_tokens': 457, 'output_tokens': 29, 'total_tokens': 486, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1 = llm_with_tools.invoke(messages)\n",
    "response_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the LLM's response to your conversation history. After receiving the response from the language model (which contains the tool call to extract the video ID), append it to your messages list to maintain the conversation context. This builds up the chat history that will be used for subsequent interactions with the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting tool call information\n",
    "After receiving the LLM's response, you need to extract the structured tool call information. The line tool_calls_1 = response_1.tool_calls gets the tool call objects that contain which tool the LLM has decided to use and what parameters to pass to it. This information will be used to execute the appropriate tool with the correct inputs.\n",
    "\n",
    "#### Creating a tool mapping dictionary\n",
    "\n",
    "Now you'll create a dictionary that maps tool names to their corresponding function objects. This mapping will be useful later when you need to programmatically invoke specific tools based on their names. It allows you to easily look up and execute a tool function when you have only the tool name as a string, which will be important when processing tool calls from the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_mapping = {\n",
    "    \"get_thumbnails\" : get_thumbnails,\n",
    "    \"get_trending_videos\": get_trending_videos,\n",
    "    \"extract_video_id\": extract_video_id,\n",
    "    \"fetch_transcript\": fetch_transcript,\n",
    "    \"search_youtube\": search_youtube,\n",
    "    \"get_full_metadata\": get_full_metadata\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the tool calls from the language model's response. When the LLM determines it needs to use one of your tools, it includes structured \"tool_calls\" in its response. Here, you're accessing those tool calls to see which tools the model decided to use in order to fulfill the request about summarizing the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "args": {
         "url": "https://www.youtube.com/watch?v=T-D1OfcDW1M"
        },
        "id": "call_JuiezhAMJKjs82avlkRJIRDV",
        "name": "extract_video_id",
        "type": "tool_call"
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_calls_1 = response_1.tool_calls\n",
    "display(JSON(tool_calls_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you're seeing the structure of the tool call that the LLM decided to make. The tool call is formatted as a dictionary with the following key components:\n",
    "\n",
    "1. `name`: 'extract_video_id' - This identifies which tool the LLM wants to use first (the video ID extraction tool)\n",
    "2. `args`: Contains the arguments to pass to the tool - in this case, the YouTube URL from your query\n",
    "3. `id`: A unique identifier for this specific tool call, which helps track the request/response pair\n",
    "4. `type`: Indicates this is a tool call rather than other types of AI responses\n",
    "\n",
    "This shows that the LLM correctly understood it needs to first extract the video ID from the URL before it can proceed with summarizing the video content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the name of the first tool that the LLM decided to use. Here you're extracting just the name component `('extract_video_id')` from the first tool call in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_video_id\n"
     ]
    }
   ],
   "source": [
    "tool_name=tool_calls_1[0]['name']\n",
    "print(tool_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need a tool ID to help the LLM know where the output came from:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_JuiezhAMJKjs82avlkRJIRDV\n"
     ]
    }
   ],
   "source": [
    "tool_call_id =tool_calls_1[0]['id']\n",
    "print(tool_call_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the arguments that need to be passed to the chosen tool. Here, you're extracting the arguments component from the first tool call, which contains the YouTube URL that needs to be processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://www.youtube.com/watch?v=T-D1OfcDW1M'}\n"
     ]
    }
   ],
   "source": [
    "args=tool_calls_1[0]['args']\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the LLM's response to your conversation history. After receiving the response from the language model (which contains the tool call to extract the video ID), you append it to your messages list to maintain the conversation context. This builds up the chat history that will be used for subsequent interactions with the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the tool call that the LLM requested. Here, you're using your tool mapping dictionary to:\n",
    "1. Look up the appropriate function based on the tool name ('extract_video_id')\n",
    "2. Call that function with the arguments provided by the LLM\n",
    "3. Capture the output (the extracted video ID)\n",
    "\n",
    "This shows how you can programmatically execute the tools that the LLM decided to use. First, you get the tool from ```tool_mapping```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tool=tool_mapping[tool_calls_1[0]['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll then call the tool with the arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T-D1OfcDW1M'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id =my_tool.invoke(tool_calls_1[0]['args'])\n",
    "video_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the tool's output to your conversation history. You'll create a `ToolMessage` that contains:\n",
    "1. The result from executing the tool (the extracted video ID)\n",
    "2. The original tool call ID to link this response back to the specific request\n",
    "\n",
    "By appending this message to your conversation history, you're informing the LLM about the results of the tool execution, which it can use in its next response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(ToolMessage(content = video_id, tool_call_id = tool_calls_1[0]['id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send your updated conversation to the LLM and store its new response. Now that you've informed the model about the extracted video ID, invoke it again to continue the process. The model will see both the original query and the result of the video ID extraction, allowing it to determine the next step needed to summarize the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_NocnjgEtmrFbXYLGv70Shntp', 'function': {'arguments': '{\"video_id\":\"T-D1OfcDW1M\",\"language\":\"en\"}', 'name': 'fetch_transcript'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 503, 'total_tokens': 530, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CUY8C226mTTzy3jc8TfnSSzuLEm0P', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--07b6b235-992e-4b25-a183-807458096aed-0', tool_calls=[{'name': 'fetch_transcript', 'args': {'video_id': 'T-D1OfcDW1M', 'language': 'en'}, 'id': 'call_NocnjgEtmrFbXYLGv70Shntp', 'type': 'tool_call'}], usage_metadata={'input_tokens': 503, 'output_tokens': 27, 'total_tokens': 530, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2 = llm_with_tools.invoke(messages)\n",
    "response_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a AI messege! Send your updated conversation to the LLM and store its new response. Now that you've informed the model about the extracted video ID, you'll invoke it again to continue the process. The model will see both the original query and the result of the video ID extraction, allowing it to determine the next step needed to summarize the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the tool calls from the language model's second response. After receiving the video ID, the LLM will likely decide to use another tool to help with the summarization task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'fetch_transcript',\n",
       "  'args': {'video_id': 'T-D1OfcDW1M', 'language': 'en'},\n",
       "  'id': 'call_NocnjgEtmrFbXYLGv70Shntp',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls_2 = response_2.tool_calls\n",
    "tool_calls_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that the LLM has decided to use the `fetch_transcript` tool as its next step. \n",
    "\n",
    "The model is passing two arguments to the transcript fetching tool:\n",
    "1. `video_id`: 'T-D1OfcDW1M' - The ID that was extracted from the original YouTube URL\n",
    "2. `language`: 'en' - Requesting the transcript in English as specified in the user's query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Fetching the transcript using the video ID obtained in the previous step. Here, you're executing the second tool that the LLM requested by:\n",
    "1. Looking up the appropriate function `('fetch_transcript')` from your tool mapping\n",
    "2. Invoking it with the video ID and language parameters\n",
    "3. Storing the resulting transcript content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models. They are everywhere. They get some things amazingly right and other things very interestingly wrong. My name\\xa0is Marina Danilevsky. I am a Senior Research Scientist here at IBM Research. And I want\\xa0to tell you about a framework to help large language models be more accurate and more up to\\xa0date: Retrieval-Augmented Generation, or RAG. Let\\'s just talk about the \"Generation\" part for a\\xa0minute. So forget the \"Retrieval-Augmented\". So the\\xa0generation, this refers to large language models,\\xa0or LLMs, that generate text in response to a user query, referred to as a prompt. These\\xa0models can have some undesirable behavior. I want to tell you an anecdote to illustrate this. So my kids, they recently asked me this question: \"In our solar system, what planet has the most\\xa0moons?\" And my response was, “Oh, that\\'s really great that you\\'re asking this question. I loved\\xa0space when I was your age.” Of course, that was like 30 years ago. But I know this! I read an\\xa0article and the article said that it was Jupiter and 88 moons. So that\\'s the answer. Now, actually,\\xa0there\\'s a couple of things wrong with my answer. First of all, I have no source to support what\\xa0I\\'m saying. So even though I confidently said “I read an article, I know the answer!”, I\\'m not\\xa0sourcing it. I\\'m giving the answer off the top of my head. And also, I actually haven\\'t kept up with\\xa0this for awhile, and my answer is out of date. So we have two problems here. One is no source.\\xa0And the second problem is that I am out of date.\\xa0\\xa0 And these, in fact, are two behaviors that are\\xa0often observed as problematic when interacting with large language models. They’re LLM\\xa0challenges. Now, what would have happened if I\\'d taken a beat and first gone and looked\\xa0up the answer on a reputable source like NASA? Well, then I would have been able to say, “Ah,\\xa0okay! So the answer is Saturn with 146 moons.” And in fact, this keeps changing because scientists\\xa0keep on discovering more and more moons. So I have now grounded my answer in something more\\xa0\\nbelievable. I have not hallucinated or made up an answer. Oh, by the way, I didn\\'t leak personal\\xa0information about how long ago it\\'s been since I was obsessed with space. All right, so what does\\xa0this have to do with large language models? Well, how would a large language model have answered\\xa0this question? So let\\'s say that I have a user asking this question about moons. A large language\\xa0model would confidently say, OK, I have been trained and from what I know in my parameters\\xa0during my training, the answer is Jupiter. The answer is wrong. But, you know, we don\\'t know. The large language model is very confident in what it answered. Now, what happens when you add this\\xa0retrieval augmented part here? What does that mean? That means that now, instead of just relying\\xa0on what the LLM knows, we are adding a content store. This could be open like the internet. This\\xa0can be closed like some collection of documents, collection of policies, whatever. The point,\\xa0though, now is that the LLM first goes and talks to the content store and says,\\xa0“Hey, can you retrieve for me information that is relevant to what the user\\'s\\xa0query was?” And now, with this retrieval-augmented answer, it\\'s not Jupiter anymore. We know that\\xa0it is Saturn. What does this look like? Well, first user prompts the LLM\\xa0with their question. They say, this is what my question was. And originally,\\xa0if we\\'re just talking to a generative model, the generative model says, “Oh, okay, I know\\xa0the response. Here it is. Here\\'s my response.”\\xa0\\xa0 But now in the RAG framework, the generative\\xa0model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s question and only then generate the\\xa0answer.\" So the prompt now has three parts: the instruction to pay attention to, the retrieved\\xa0content, together with the user\\'s question. Now give a response. And in fact, now you can give\\xa0evidence for why your response was what it was.\\xa0\\xa0 So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll start with the out of\\xa0date part. Now, instead of having to retrain your model, if new information comes up, like, hey,\\xa0we found some more moons-- now to Jupiter again, maybe it\\'ll be Saturn again in the future. All\\xa0you have to do is you augment your data store with new information, update information. So now the next time that a user comes and asks the question, we\\'re ready. We just go ahead and retrieve the most up to date information. The second problem, source. Well, the large language model is now being instructed to pay attention to primary source data before giving its response. And in fact, now being able to give evidence. This makes it less likely to hallucinate or to leak data because it is less likely to rely only on information that it learned during training. It also allows us to get the model to have a behavior that can be very positive, which is knowing when to say, “I don\\'t know.” If\\xa0the user\\'s question cannot be reliably answered based on your data store, the model should say,\\xa0\"I don\\'t know,\" instead of making up something that is believable and may mislead the user. This\\xa0can have a negative effect as well though, because if the retriever is not sufficiently\\xa0good to give the large language model the best, most high-quality grounding information, then\\xa0maybe the user\\'s query that is answerable doesn\\'t get an answer. So this is actually why lots\\xa0of folks, including many of us here at IBM, are working the problem on both sides. We are both\\xa0working to improve the retriever to give the large language model the best quality data on which\\xa0to ground its response, and also the generative part so that the LLM can give the richest, best\\xa0response finally to the user when it generates the answer. Thank you for learning more about RAG\\xa0and like and subscribe to the channel. Thank you.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_transcript_tool_output = tool_mapping[tool_calls_2[0]['name']].invoke(tool_calls_2[0]['args'])\n",
    "fetch_transcript_tool_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "You're adding the transcript content to your conversation history by creating another `ToolMessage` that contains the transcript text and the ID of the tool call that requested it. This gives the LLM access to the actual video content so it can generate a summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(ToolMessage(content = fetch_transcript_tool_output, tool_call_id = tool_calls_2[0]['id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the final summary by sending your complete conversation history to the LLM. Now that the model has access to both the video ID and the full transcript, you'll invoke it one more time to generate the summary that the user requested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The video features Marina Danilevsky, a Senior Research Scientist at IBM Research, discussing \"Retrieval-Augmented Generation\" (RAG), a framework aimed at improving the accuracy and currency of large language models (LLMs).\\n\\n### Summary:\\n1. **Introduction to LLM Challenges**: Danilevsky highlights two main issues with LLMs: they often lack sources for their answers and can provide outdated information.\\n\\n2. **Illustrative Anecdote**: She shares a personal story about answering her children\\'s question about which planet has the most moons. Her initial answer, based on memory, was wrong and lacked credible sourcing.\\n\\n3. **RAG Framework Explanation**: \\n   - The \"Retrieval\" function allows LLMs to access a content store (like the internet or a database) to find accurate answers instead of relying solely on their training data.\\n   - The LLM first retrieves relevant information before generating a response, combining this new data with the user\\'s question for a more informed and accurate answer.\\n\\n4. **Benefits of RAG**:\\n   - Quickly updates the model\\'s knowledge base without requiring full retraining when new information arises.\\n   - Requires LLMs to source their information, reducing the chances of providing incorrect answers.\\n   - Facilitates a positive behavior where the model can admit when it doesn\\'t know the answer rather than create misleading content.\\n\\n5. **Collaborative Effort**: The video emphasizes the ongoing work to enhance both the retrieval mechanism and the generative aspect of LLMs to improve the quality of answers provided.\\n\\nDanilevsky concludes by encouraging viewers to subscribe for more information about RAG.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 333, 'prompt_tokens': 1889, 'total_tokens': 2222, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CUY8rFStsMBfLi3XG7el9mK6LISFw', 'finish_reason': 'stop', 'logprobs': None}, id='run--c6774bef-9427-4086-8d6a-5bf00c35ede0-0', usage_metadata={'input_tokens': 1889, 'output_tokens': 333, 'total_tokens': 2222, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating the tool calling process\n",
    "\n",
    "You manually saw how you input a text request to your LLM, where the LLM recognized that a tool call was required. Then, you extracted the tool content, formatted the input, made the next tool call, and repeated these steps. While this step-by-step approach helps understand the process, it would be tedious to implement for every application. Now let's automate this entire workflow.\n",
    "\n",
    "#### Extracting tool information from LLM response\n",
    "Create a function to automate tool calling. The input is the tool call object from which you extract the name, and use the tool_mapping dictionary to find the correct function to call. You'll pass the arguments from the tool call to this function and then send the output back as a ToolMessage with the tool_call_id included.\n",
    "The tool_call_id is an essential part of this process as it links each tool response back to the specific tool request made by the language model. This ID ensures the LLM can match responses to its requests, which is crucial when multiple tools are called in sequence or simultaneously. Without this ID, the LLM would have no way to know which response corresponds to which request, making multi-step reasoning impossible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing steps\n",
    "def execute_tool(tool_call):\n",
    "    \"\"\"Execute single tool call and return ToolMessage\"\"\"\n",
    "    try:\n",
    "        result = tool_mapping[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        return ToolMessage(\n",
    "            content=str(result),\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return ToolMessage(\n",
    "            content=f\"Error: {str(e)}\",\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to chain all your functions or tools together, but before you do so, you need to format the data properly. Not only are you required to store the output of each tool, but you also need to store state information like tool IDs. To do this effectively, you must ensure the output of each tool can be properly passed to the next step in your pipeline. The RunnablePassthrough component allows you to maintain state throughout the chain while adding or transforming data at each step, making it ideal for connecting your various tools into a cohesive workflow.\n",
    "The RunnableLambda, placed at the end of your chain, serves a different purpose - it extracts only the final result you want to present to the user. After all the tool calls and message processing, you have a rich state object with many fields, but the user typically only needs the final answer. The RunnableLambda transforms this complete state into just the information you want to return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the summarization chain\n",
    "\n",
    "Now, you'll combine your functions into a complete `summarization_chain` using the pipe operator `|`, which applies functions sequentially (similar to function composition where `f|g(x)` is equivalent to `f(g(x))`).\n",
    "\n",
    "The workflow follows these steps:\n",
    "1. Convert the input prompt to a HumanMessage\n",
    "2. Pass the message to LLM with tools\n",
    "3. Extract tool calls from LLM response\n",
    "4. Update message history with tool results\n",
    "5. Send updated messages back to LLM\n",
    "6. Repeat steps 3-5 as needed\n",
    "7. Finally, extract just the content from the final message using RunnableLambda\n",
    "\n",
    "Each step maintains state using RunnablePassthrough until you reach the final message, at which point you'll apply RunnableLambda to extract only the summary text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_chain = (\n",
    "    # Start with initial query\n",
    "    RunnablePassthrough.assign(\n",
    "        messages=lambda x: [HumanMessage(content=x[\"query\"])]\n",
    "    )\n",
    "    # First LLM call (extract video ID)\n",
    "    | RunnablePassthrough.assign(\n",
    "        ai_response=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    "    )\n",
    "    # Process first tool call\n",
    "    | RunnablePassthrough.assign(\n",
    "        tool_messages=lambda x: [\n",
    "            execute_tool(tc) for tc in x[\"ai_response\"].tool_calls\n",
    "        ]\n",
    "    )\n",
    "    # Update message history\n",
    "    | RunnablePassthrough.assign(\n",
    "        messages=lambda x: x[\"messages\"] + [x[\"ai_response\"]] + x[\"tool_messages\"]\n",
    "    )\n",
    "    # Second LLM call (fetch transcript)\n",
    "    | RunnablePassthrough.assign(\n",
    "        ai_response2=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    "    )\n",
    "    # Process second tool call\n",
    "    | RunnablePassthrough.assign(\n",
    "        tool_messages2=lambda x: [\n",
    "            execute_tool(tc) for tc in x[\"ai_response2\"].tool_calls\n",
    "        ]\n",
    "    )\n",
    "    # Final message update\n",
    "    | RunnablePassthrough.assign(\n",
    "        messages=lambda x: x[\"messages\"] + [x[\"ai_response2\"]] + x[\"tool_messages2\"]\n",
    "    )\n",
    "    # Generate final summary\n",
    "    | RunnablePassthrough.assign(\n",
    "        summary=lambda x: llm_with_tools.invoke(x[\"messages\"]).content\n",
    "    )\n",
    "    # Return just the summary text\n",
    "    | RunnableLambda(lambda x: x[\"summary\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you invoke the summarization chain with a YouTube video URL; this passes your query containing a YouTube URL to the chain, which automatically extracts the video ID, fetches the transcript, and generates a summary of the content.\n",
    "\n",
    "**Note: If you find any issues with the given video link below, try any Youtube video link of your choosing.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Summary:\n",
      " The YouTube video covers the controversial 1959 psychological experiment known as \"The Three Christs of Ipsilanti,\" conducted by psychologist Milton Rock. In this experiment, Rock housed three men diagnosed with paranoid schizophrenia, each of whom believed they were Jesus Christ. The intent was to explore the dynamics of their delusions by having them live together and interact daily.\n",
      "\n",
      "Key points from the video include:\n",
      "\n",
      "1. **Background and Setup**: Rock aimed to study how individuals with deep-rooted identity crises could potentially influence each other. He selected three patients—Clyde, Joseph, and Leon—whose differing backgrounds and mental health histories provided a diverse set of delusions.\n",
      "\n",
      "2. **Patient Profiles**:\n",
      "   - **Clyde**: A former farmer whose life unraveled after personal tragedies. He became convinced he was God following a breakdown.\n",
      "   - **Joseph**: A writer with obsessional thoughts who believed he was from England. His paranoia intensified during the study.\n",
      "   - **Leon**: The youngest, raised by a devoutly religious mother, who developed a complex mythology about his identity, including replacing his fictional relationship with Yeti Woman by claiming various divine characteristics.\n",
      "\n",
      "3. **Interactions and Experiments**: Initially, the three men had combative interactions, constantly vying to assert their identities. Over time, moments of connection arose, including shared experiences and expressions of care, particularly Leon showing compassion towards Clyde.\n",
      "\n",
      "4. **Controversial Methods**: Later in the experiment, as apparent progress stagnated, Rock began sending letters to the men from fictional authority figures, which was ethically contentious. These letters aimed to elicit changes in the men’s beliefs by instilling trust in these created identities.\n",
      "\n",
      "5. **Results and Reflections**: None of the men were cured; they returned to their delusions, albeit with insights gained about their identities. The experiment raised significant ethical questions applicable to modern psychology, igniting discussions on the balance of care versus experimentation.\n",
      "\n",
      "6. **Outcome for Patients**: By the conclusion of the study in 1961, Clyde had been released to live with his family, Joseph died in 1976, and Leon remained a patient at the hospital. Rock reflected on the experience as he felt he had failed to change the men but learned profound lessons about humanity and identity.\n",
      "\n",
      "The video balances humor and tragedy, exploring mental illness while acknowledging the real lives involved in this controversial experiment.\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "result = summarization_chain.invoke({\n",
    "    \"query\": \"Summarize this YouTube video: https://www.youtube.com/watch?v=t97ipSIDEfU\"\n",
    "})\n",
    "\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Up to this point, you've demonstrated how to manually orchestrate the tool calling process step by step. You first invoked the LLM with the user's query, interpreted its decision to use the `extract_video_id` tool, executed that tool, fed the result back to the LLM, processed its next decision to use the `fetch_transcript` tool, executed that tool, and finally had the LLM generate a summary based on the transcript.\n",
    "\n",
    "Now you'll see how to accomplish the same workflow more efficiently using LangChain's chain functionality, which automates this back-and-forth process of tool selection, execution, and response handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the initial message setup\n",
    "\n",
    "Here you're setting up the first step of your chain that will handle the initial user query. The `RunnablePassthrough.assign` creates a component that takes an input dictionary containing a \"query\" and converts it into a list containing a single `HumanMessage` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_setup = RunnablePassthrough.assign(\n",
    "    messages=lambda x: [HumanMessage(content=x[\"query\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the first LLM interaction\n",
    "\n",
    "Here, you'll create the second step of your chain, which handles the first interaction with the language model. This component takes the formatted messages from the previous step, sends them to your tool-equipped LLM, and captures the response in a field called \"ai_response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_llm_call = RunnablePassthrough.assign(\n",
    "    ai_response=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the first tool call\n",
    "\n",
    "Here, you're defining the processing step that handles the LLM's first tool call. This component:\n",
    "1. Executes each tool call by passing it to your `execute_tool` function, which runs the appropriate tool and returns the result as a `ToolMessage`\n",
    "2. Updates the message history by combining the original messages, the LLM's response, with the tool calls, and the tool results\n",
    "3. Prepares the updated conversation state for the next interaction with the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tool_processing = RunnablePassthrough.assign(\n",
    "    tool_messages=lambda x: [\n",
    "        execute_tool(tc) for tc in x[\"ai_response\"].tool_calls\n",
    "    ]\n",
    ").assign(\n",
    "    messages=lambda x: x[\"messages\"] + [x[\"ai_response\"]] + x[\"tool_messages\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the second LLM interaction\n",
    "\n",
    "Here, you're creating the next step in your chain that handles the second interaction with the language model. This component takes the updated message history (which now includes the results from the first tool call) and sends it to the LLM again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_llm_call = RunnablePassthrough.assign(\n",
    "    ai_response2=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the second tool call\n",
    "\n",
    "Here, you're defining the processing step that handles the LLM's second tool call. Similar to the first tool processing step, this component executes the tool calls (typically fetching the transcript), creates tool messages with the results, and updates the message history by combining everything for the final summarization step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tool_processing = RunnablePassthrough.assign(\n",
    "    tool_messages2=lambda x: [\n",
    "        execute_tool(tc) for tc in x[\"ai_response2\"].tool_calls\n",
    "    ]\n",
    ").assign(\n",
    "    messages=lambda x: x[\"messages\"] + [x[\"ai_response2\"]] + x[\"tool_messages2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the final summary\n",
    "\n",
    "Here, you're defining the final step that produces the summary of the YouTube video. This component:\n",
    "1. Takes the complete message history (which now contains the original query, tool calls, and tool results)\n",
    "2. Invokes the LLM one last time to generate a summary\n",
    "3. Extracts just the content field from the LLM's response\n",
    "4. Uses a RunnableLambda to return only the summary text as the final output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = RunnablePassthrough.assign(\n",
    "    summary=lambda x: llm_with_tools.invoke(x[\"messages\"]).content\n",
    ") | RunnableLambda(lambda x: x[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assembling the complete chain\n",
    "\n",
    "Now, you're combining all the individual components you've defined into a single cohesive chain. By piping each step to the next, you'll create a workflow that:\n",
    "1. Formats the initial query\n",
    "2. Gets the first LLM response (video ID extraction)\n",
    "3. Processes the first tool call\n",
    "4. Gets the second LLM response (transcript request)\n",
    "5. Processes the second tool call\n",
    "6. Generates the final summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    initial_setup\n",
    "    | first_llm_call\n",
    "    | first_tool_processing\n",
    "    | second_llm_call\n",
    "    | second_tool_processing\n",
    "    | final_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you're testing your automated chain with the original video summarization query you handled manually before. By passing in the same query to your chain, you can confirm that it produces the same results but in a much more streamlined manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Summary:\n",
      " The video features Marina Danilevsky, a Senior Research Scientist at IBM Research, discussing a framework called Retrieval-Augmented Generation (RAG) designed to improve the accuracy and relevance of large language models (LLMs).\n",
      "\n",
      "### Key Points:\n",
      "1. **Introduction to LLM Challenges**:\n",
      "   - LLMs often provide answers that are correct but can also be outdated or lack proper sourcing.\n",
      "   - Example given: A common inquiry about which planet has the most moons. The speaker mentions that confidently providing outdated information (like attributing it to Jupiter instead of Saturn) is a common issue with LLMs.\n",
      "\n",
      "2. **The Problem with Generation**:\n",
      "   - Traditionally, LLMs generate responses based solely on their trained data, which can lead to inaccuracies.\n",
      "\n",
      "3. **Retrieval-Augmented Generation (RAG)**:\n",
      "   - RAG enhances the model's capability by incorporating a retrieval system. Instead of relying solely on internal knowledge, it first retrieves relevant information from a content store (either open like the internet or specific documents).\n",
      "   - By doing this, the model can generate more accurate responses based on updated data.\n",
      "\n",
      "4. **Two Main Improvements**:\n",
      "   - **Outdated Information**: RAG allows for updating the information as new data becomes available without retraining the entire model.\n",
      "   - **Credibility and Sourcing**: The model is instructed to base its answers on primary sources, minimizing hallucinations (false information) and improving reliability. It can also recognize when there isn't enough information to provide an answer.\n",
      "\n",
      "5. **Future Work**:\n",
      "   - The goal is to enhance both the retrieval mechanism and the generative capabilities of LLMs to ensure that users receive high-quality responses.\n",
      "\n",
      "In summary, the RAG framework seeks to address common deficiencies in LLMs by integrating real-time data retrieval and ensuring responses are well-grounded in credible sources.\n"
     ]
    }
   ],
   "source": [
    "query = {\"query\": \"I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\"}\n",
    "result = summarization_chain.invoke(query)\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Chain with a Different Query\n",
    "\n",
    "Here, you're testing your completed chain with a new query to demonstrate its flexibility. Instead of requesting a video summary, you're asking for information about trending videos in India. You'll create a dictionary with the query and invoke your chain, which will handle all the necessary tool calls automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube:tab] trending: The channel/playlist does not exist and the URL redirected to youtube.com home page\n",
      "ERROR: [youtube:tab] trending: The channel/playlist does not exist and the URL redirected to youtube.com home page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Summary:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "query = {\"query\": \"Get top 3 youtube videos in India and their metadata\"}\n",
    "result = summarization_chain.invoke(query)\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive chain flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've created a chain that works well for your specific two-step tool calling process, you need to consider more complex scenarios. Your current chain is limited to exactly two tool calls in a fixed sequence. In real-world applications, you might need a variable number of tool calls depending on the user's query - for example, getting trending videos and then fetching metadata for each video, or searching for videos on a topic and then getting transcripts for multiple results.\n",
    "\n",
    "To handle these more complex scenarios, you'll build a recursive chain that can dynamically decide how many tool calls are needed and continue processing until all necessary information has been gathered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "import json\n",
    "\n",
    "def execute_tool(tool_call):\n",
    "    \"\"\"Execute single tool call and return ToolMessage\"\"\"\n",
    "    try:\n",
    "        result = tool_mapping[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        content = json.dumps(result) if isinstance(result, (dict, list)) else str(result)\n",
    "    except Exception as e:\n",
    "        content = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return ToolMessage(\n",
    "        content=content,\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the core processing logic\n",
    "\n",
    "This function handles the core processing logic of your recursive chain. It takes the current conversation history and:\n",
    "\n",
    "1. Identifies the most recent message in the conversation\n",
    "2. Extracts all tool calls from that message and executes them in parallel using your `execute_tool` helper\n",
    "3. Updates the message history by adding the tool response messages\n",
    "4. Gets the next response from the language model based on the updated conversation\n",
    "5. Returns the complete updated message history with both tool responses and the new LLM response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tool_calls(messages):\n",
    "    \"\"\"Recursive tool call processor\"\"\"\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Execute all tool calls in parallel\n",
    "    tool_messages = [\n",
    "        execute_tool(tc) \n",
    "        for tc in getattr(last_message, 'tool_calls', [])\n",
    "    ]\n",
    "    \n",
    "    # Add tool responses to message history\n",
    "    updated_messages = messages + tool_messages\n",
    "    \n",
    "    # Get next LLM response\n",
    "    next_ai_response = llm_with_tools.invoke(updated_messages)\n",
    "    \n",
    "    return updated_messages + [next_ai_response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the recursive stopping condition\n",
    "\n",
    "This function determines whether your recursive process should continue or terminate. It:\n",
    "\n",
    "1. Takes the current message history and examines the last message\n",
    "2. Checks if that message contains any tool calls using the `getattr` function (which safely handles cases where the attribute might not exist)\n",
    "3. Returns a boolean value - `True` if there are more tool calls to process, and `False` when you reach a point where the LLM has provided a final answer without requesting additional tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(messages):\n",
    "    \"\"\"Check if you need another iteration\"\"\"\n",
    "    last_message = messages[-1]\n",
    "    return bool(getattr(last_message, 'tool_calls', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Implementing the recursive function\n",
    "\n",
    "This function implements the actual recursion that powers your dynamic tool calling process:\n",
    "\n",
    "1. It first checks the stopping condition using the `should_continue` function to determine if more tools need to be called\n",
    "2. If more tool calls are needed, it processes those calls using your `process_tool_calls` function and then recursively calls itself with the updated messages\n",
    "3. If no more tool calls are needed, it returns the final message history, which contains the complete conversation, including the LLM's final response\n",
    "\n",
    "After defining this recursive function, you'll wrap it in a `RunnableLambda` to make it compatible with LangChain's chain architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recursive_chain(messages):\n",
    "    \"\"\"Recursively process tool calls until completion\"\"\"\n",
    "    if should_continue(messages):\n",
    "        new_messages = process_tool_calls(messages)\n",
    "        return _recursive_chain(new_messages)\n",
    "    return messages\n",
    "\n",
    "recursive_chain = RunnableLambda(_recursive_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the complete universal chain\n",
    "\n",
    "Now, you're assembling your final universal chain that can handle any type of query requiring any number of tool calls. This chain consists of three main steps:\n",
    "\n",
    "1. The first step converts the user query into a properly formatted `HumanMessage` object\n",
    "2. The second step sends this initial message to your tool-equipped LLM and adds the LLM's first response to the message history\n",
    "3. The final step passes the conversation to your recursive chain, which will handle all subsequent tool calls until the LLM provides a final answer\n",
    "\n",
    "This universal chain is much more flexible than your earlier fixed-step chain, as it can dynamically adapt to queries that require different numbers and types of tool calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_chain = (\n",
    "    RunnableLambda(lambda x: [HumanMessage(content=x[\"query\"])])\n",
    "    | RunnableLambda(lambda messages: messages + [llm_with_tools.invoke(messages)])\n",
    "    | recursive_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube:tab] trending: The channel/playlist does not exist and the URL redirected to youtube.com home page\n",
      "ERROR: [youtube:tab] trending: The channel/playlist does not exist and the URL redirected to youtube.com home page\n",
      "ERROR: [youtube:tab] trending: The channel/playlist does not exist and the URL redirected to youtube.com home page\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ee45c3217c0>, 'Connection to rr4---sn-q4flrney.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ee45d81b5f0>, 'Connection to rr4---sn-q4flrney.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ee45d8dd940>, 'Connection to rr4---sn-q4flrney.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ee453a2d310>, 'Connection to rr4---sn-q4flrney.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ee453a2cce0>, 'Connection to rr4---sn-q4flrney.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ee453a2ca70>, 'Connection to rr4---sn-q4flrney.googlevideo.com timed out. (connect timeout=20.0)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here are the top 3 trending videos in the US along with their metadata and thumbnails:\\n\\n### 1. Spotify VIRAL Music 2025 ~ Top Spotify Songs Mix 🎧 Trending Songs 2025 ~ Best Pop Hits Playlist 2025\\n- **Channel:** Dynamic Deep House\\n- **Views:** 45,186\\n- **Duration:** 156 minutes and 1 second (9361 seconds)\\n- **Likes:** 489\\n- **Comments:** 13\\n- **[Watch Video](https://youtu.be/fGPSs1PLc9o)**\\n- **Thumbnails:**\\n  ![Thumbnail 1](https://i.ytimg.com/vi/fGPSs1PLc9o/3.jpg)\\n  ![Thumbnail 2](https://i.ytimg.com/vi/fGPSs1PLc9o/1.jpg)\\n  ![Thumbnail 3](https://i.ytimg.com/vi/fGPSs1PLc9o/hqdefault.jpg)\\n\\n### 2. Who did it best? ❤️ #Shorts #Dance #Trending\\n- **Channel:** Brat TV\\n- **Views:** 8,690,096\\n- **Duration:** 15 seconds\\n- **Likes:** 109,344\\n- **Comments:** 2,000\\n- **[Watch Video](https://youtu.be/2sokLXhqRFQ)**\\n- **Thumbnails:**\\n  ![Thumbnail 1](https://i.ytimg.com/vi/2sokLXhqRFQ/3.jpg)\\n  ![Thumbnail 2](https://i.ytimg.com/vi/2sokLXhqRFQ/1.jpg)\\n  ![Thumbnail 3](https://i.ytimg.com/vi/2sokLXhqRFQ/hqdefault.jpg)\\n\\n### 3. Top 17 New Technology Trends That Will Define 2026\\n- **Channel:** AI Uncovered\\n- **Views:** 336,910\\n- **Duration:** 12 minutes and 9 seconds (729 seconds)\\n- **Likes:** 4,668\\n- **Comments:** 302\\n- **[Watch Video](https://youtu.be/Otim2mDjsYM)**\\n- **Thumbnails:**\\n  ![Thumbnail 1](https://i.ytimg.com/vi/Otim2mDjsYM/3.jpg)\\n  ![Thumbnail 2](https://i.ytimg.com/vi/Otim2mDjsYM/1.jpg)\\n  ![Thumbnail 3](https://i.ytimg.com/vi/Otim2mDjsYM/hqdefault.jpg)\\n\\nLet me know if you need more information!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 564, 'prompt_tokens': 10628, 'total_tokens': 11192, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2560}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CUYzdopnJADg92a0Jr95CXrBcv1bv', 'finish_reason': 'stop', 'logprobs': None} id='run--0c5db7f4-8b89-466c-b75d-58ba5238d908-0' usage_metadata={'input_tokens': 10628, 'output_tokens': 564, 'total_tokens': 11192, 'input_token_details': {'audio': 0, 'cache_read': 2560}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(universal_chain.invoke({\n",
    "    \"query\": \"Show top 3 US trending videos with metadata and thumbnails\"\n",
    "})[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Try a different video with a Youtube link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "video_id =\"6LtrJD5Dz40\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_id = \"INSERT_VIDEO_ID_HERE\"  # Replace with the actual video ID\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Extract the video ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted video ID: 6LtrJD5Dz40\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "youtube_url = \"https://www.youtube.com/watch?v=6LtrJD5Dz40\"\n",
    "video_id = extract_video_id.run(youtube_url)\n",
    "print(f\"Extracted video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_id = extract_video_id.run(youtube_url)\n",
    "print(f\"Extracted video ID: {video_id}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Collect all necessary data about the video in one go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved metadata for: The BMAD Builder is Here - V6 Alpha: 5 Major Features (Download Now)\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "video_metadata = get_full_metadata.run(youtube_url)\n",
    "print(f\"Retrieved metadata for: {video_metadata['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_metadata = get_full_metadata.run(youtube_url)\n",
    "print(f\"Retrieved metadata for: {video_metadata['title']}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Get video transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved transcript with 23260 characters\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "transcript = fetch_transcript.run(video_id)\n",
    "print(f\"Retrieved transcript with {len(transcript)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "transcript = fetch_transcript.run(video_id)\n",
    "print(f\"Retrieved transcript with {len(transcript)} characters\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Get video thumbnails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 46 thumbnails\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "thumbnails = get_thumbnails.run(youtube_url)\n",
    "print(f\"Retrieved {len(thumbnails)} thumbnails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video thumbnails\n",
    "thumbnails = get_thumbnails.run(youtube_url)\n",
    "print(f\"Retrieved {len(thumbnails)} thumbnails\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a comprehensive prompt to be passed to LLM to generate a summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Please analyze this YouTube video and provide a comprehensive summary.\n",
    "\n",
    "VIDEO TITLE: {video_metadata['title']}\n",
    "CHANNEL: {video_metadata['channel']}\n",
    "VIEWS: {video_metadata['views']}\n",
    "DURATION: {video_metadata['duration']} seconds\n",
    "LIKES: {video_metadata['likes']}\n",
    "\n",
    "TRANSCRIPT EXCERPT:\n",
    "{transcript[:3000]}... (transcript truncated for brevity)\n",
    "\n",
    "Based on this information, please provide:\n",
    "1. A concise summary of the video content (3-5 bullet points)\n",
    "2. The main topics or themes discussed\n",
    "3. The intended audience for this content\n",
    "4. A brief analysis of why this video might be performing well (or not)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for prompt</summary>\n",
    "\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "Please analyze this YouTube video and provide a comprehensive summary.\n",
    "\n",
    "VIDEO TITLE: {video_metadata['title']}\n",
    "CHANNEL: {video_metadata['channel']}\n",
    "VIEWS: {video_metadata['views']}\n",
    "DURATION: {video_metadata['duration']} seconds\n",
    "LIKES: {video_metadata['likes']}\n",
    "\n",
    "TRANSCRIPT EXCERPT:\n",
    "{transcript[:3000]}... (transcript truncated for brevity)\n",
    "\n",
    "Based on this information, please provide:\n",
    "1. A concise summary of the video content (3-5 bullet points)\n",
    "2. The main topics or themes discussed\n",
    "3. The intended audience for this content\n",
    "4. A brief analysis of why this video might be performing well (or not)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Single LLM invocation with all the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=prompt)]\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "response = llm.invoke(messages)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Display the comprehensive analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VIDEO ANALYSIS =====\n",
      "\n",
      "### Summary of the Video Content:\n",
      "1. **Introduction and Milestones:** The host celebrates six months since the launch of the BMAD method (Version 1) and shares achievements such as 20,000 subscribers and 18,000 GitHub stars.\n",
      "2. **Community Growth:** Emphasizes the supportive and inclusive nature of the community surrounding the BMAD method, inviting viewers to join the Discord for support.\n",
      "3. **Announcement of V6 Alpha:** The host introduces the alpha version of the BMAD method (Version 6), highlighting its development and testing opportunities for the community.\n",
      "4. **Future Releases:** Mentions upcoming videos detailing many new features in V6, with a full beta release expected soon.\n",
      "5. **Acknowledgments and Call to Action:** Thanks supporters and urges viewers to subscribe for more content and future updates on the BMAD method.\n",
      "\n",
      "### Main Topics or Themes Discussed:\n",
      "- The evolution and growth of the BMAD method since its inception.\n",
      "- Community engagement and the importance of user feedback in development.\n",
      "- Introduction of new features in Version 6 of the BMAD method.\n",
      "- The significance of participation in alpha testing and the future direction of the project.\n",
      "- Gratitude towards the community that supports the channel and project.\n",
      "\n",
      "### Intended Audience for This Content:\n",
      "- Developers and tech enthusiasts interested in agent-based AI workflows.\n",
      "- Existing users of the BMAD method as well as potential newcomers.\n",
      "- Members of the coding and software development community looking to engage with innovative tools and resources.\n",
      "- Individuals interested in collaborative projects and open-source development.\n",
      "\n",
      "### Analysis of Video Performance:\n",
      "This video is likely performing well due to several factors:\n",
      "- **Community Engagement:** The host actively acknowledges and thanks the community, which fosters a sense of belonging and encourages more viewers to engage with the content.\n",
      "- **Clear Milestones:** Highlighting significant milestones (like subscriber count and GitHub stars) creates a narrative of success which can attract new viewers and encourage community participation.\n",
      "- **Anticipation for New Features:** The unveiling of the alpha version and promise of forthcoming videos builds excitement and encourages viewers to stay tuned for updates, which likely contributes to repeat viewership.\n",
      "- **Exclusive Opportunities:** Offering viewers a chance to engage in alpha testing can attract a more invested audience willing to contribute to the project.\n",
      "- **High Engagement Metrics:** The video has a substantial number of likes and views relative to its length, indicating strong viewer interest and approval.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== VIDEO ANALYSIS =====\\n\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "print(\"\\n===== VIDEO ANALYSIS =====\\n\")\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kunal Makwana](https://author.skills.network/instructors/kunal_makwana) is a Data Scientist at IBM and is currently pursuing his Master's in Computer Science at Dalhousie University.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "1603f145970a8c2283bae36941000a6d4b9e191c2755a7412e95925fa9d1ae18"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
