{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DataWizard: AI-Powered Data Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **45** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The challenge\n",
    "\n",
    "In today's data-driven world, valuable insights are locked away in spreadsheets and datasets that most professionals don't have the technical skills to analyze. Business owners, managers, and domain experts know their data is valuable but lack the programming knowledge to extract meaningful insights.\n",
    "\n",
    "## About this lab\n",
    "\n",
    "In this lab, you will create an AI-powered agent that can help non-technical users perform data science tasks through natural language. You will:\n",
    "\n",
    "1. Build a collection of LangChain tools that perform basic data science tasks:\n",
    "   - Listing available datasets\n",
    "   - Loading and analyzing CSV files\n",
    "   - Generating dataset summaries and statistics\n",
    "   - Training and evaluating machine learning models\n",
    "\n",
    "2. Document each tool's purpose and output format clearly to ensure the AI agent can use them effectively\n",
    "\n",
    "3. Demonstrate why regular conversational agents have limitations when working with structured data\n",
    "\n",
    "4. Implement an executor agent that can manage a multi-step workflow for data analysis\n",
    "\n",
    "By the end of this lab, you'll understand how to combine language models with specialized tools to create practical applications that make data science accessible to everyone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Langchain-tools\">Langchain tools</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Dataset-caching-tool\">Dataset caching tool</a></li>\n",
    "            <li><a href=\"#Summarization-tool\">Summarization tool</a></li>\n",
    "            <li><a href=\"#DataFrame-method-execution-tool\">DataFrame method execution tool</a></li>\n",
    "            <li><a href=\"#Model-evaluation-tools\">Model evaluation tools</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Agents\">Agents</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Agent-creation-and-limitations\">Agent creation and limitations</a></li>\n",
    "            <li><a href=\"#Agent-executor-ReAct\">Agent executor ReAct</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<li><a href=\"#Authors\">Authors</a></li>\n",
    "<li><a href=\"#Contributors\">Contributors</a></li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this project, you will be able to:\n",
    "1. **Create specialized LangChain tools**: Develop custom tools for performing key data science tasks like dataset discovery, analysis, and modeling.\n",
    "2. **Implement efficient caching mechanisms**: Build a system that stores and manages datasets in memory to optimize performance across multiple queries.\n",
    "3. **Design a natural language interface**: Connect a language model to your tools, enabling conversational access to data science capabilities.\n",
    "4. **Develop context-aware agents**: Create an executor agent that can maintain state and execute multi-step data analysis workflows.\n",
    "5. **Handle errors gracefully**: Implement robust error handling for file operations and data processing tasks.\n",
    "6. **Test your assistant with real queries**: Evaluate the system with practical business questions that demonstrate its ability to make data science accessible.\n",
    "\n",
    "This project equips you with the skills to bridge the gap between non-technical users and data insights, democratizing access to advanced analytics through conversation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "* [`langchain`](https://python.langchain.com/docs/get_started/introduction) for building modular AI applications with tools and agents.\n",
    "* [`langchain-openai`](https://python.langchain.com/docs/integrations/llms/openai) for connecting LangChain with OpenAI's language models.\n",
    "* [`openai`](https://github.com/openai/openai-python) for accessing AI models that power your conversational interface.\n",
    "* [`pandas`](https://pandas.pydata.org/) for data manipulation and analysis of CSV datasets.\n",
    "* [`numpy`](https://numpy.org/) for numerical operations and array processing.\n",
    "* [`scikit-learn`](https://scikit-learn.org/stable/) for implementing machine learning models and evaluation metrics.\n",
    "* [`matplotlib`](https://matplotlib.org/) for creating data visualizations based on analysis results.\n",
    "* [`seaborn`](https://seaborn.pydata.org/) for enhanced statistical visualizations of dataset patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them. This step could take **several minutes**; please be patient.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/crvBKBOkg9aBzXZiwGEXbw/Restarting-the-Kernel.png\" width=\"50%\" alt=\"Restart kernel\">\n",
    "\n",
    "**NOTE**: If you encounter any issues, restart the kernel and run it again by clicking the **Restart the kernel** icon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed jiter-0.11.1 langchain-core-0.3.79 langchain-openai-0.3.10 langsmith-0.4.37 openai-1.109.1 orjson-3.11.3 regex-2025.10.23 requests-toolbelt-1.0.0 tiktoken-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed langchain-0.3.21 langchain-text-splitters-0.3.11 langsmith-0.3.45\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed openai-1.68.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed numpy-2.3.4 pandas-2.2.3 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed numpy-2.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.1 pillow-12.0.0 pyparsing-3.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.6.1 scipy-1.16.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai==0.3.10 | tail -n 1\n",
    "%pip install langchain==0.3.21 | tail -n 1\n",
    "%pip install openai==1.68.2 | tail -n 1\n",
    "%pip install pandas==2.2.3 | tail -n 1\n",
    "%pip install numpy==2.2.4 | tail -n 1\n",
    "%pip install matplotlib==3.10.1 | tail -n 1\n",
    "%pip install seaborn==0.13.2 | tail -n 1\n",
    "%pip install scikit-learn==1.6.1 | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the `.csv` files required in the project later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-22 08:28:25--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/N0CceRlquaf9q85PK759WQ/regression-dataset.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 2002174 (1.9M) [text/csv]\n",
      "Saving to: â€˜regression-dataset.csvâ€™\n",
      "\n",
      "regression-dataset. 100%[===================>]   1.91M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-10-22 08:28:26 (64.4 MB/s) - â€˜regression-dataset.csvâ€™ saved [2002174/2002174]\n",
      "\n",
      "--2025-10-22 08:28:26--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7J73m6Nsz-vmojwab91gMA/classification-dataset.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 121385 (119K) [text/csv]\n",
      "Saving to: â€˜classification-dataset.csvâ€™\n",
      "\n",
      "classification-data 100%[===================>] 118.54K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2025-10-22 08:28:26 (62.1 MB/s) - â€˜classification-dataset.csvâ€™ saved [121385/121385]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/N0CceRlquaf9q85PK759WQ/regression-dataset.csv\n",
    "! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7J73m6Nsz-vmojwab91gMA/classification-dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "Import all required libraries here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import sklearn\n",
    "import langchain\n",
    "import openai\n",
    "import langchain_openai\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Disclaimer\n",
    "This lab uses LLMs provided by OpenAI. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to configure your own API keys. Please note that using your own API keys means that you will incur personal charges.\n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API key. This lab uses the `init_chat_model` function from `langchain`. To use the model you must set the environment variable `OPENAI_API_KEY` to your OpenAI API key. **DO NOT** run the cell below if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE IF YOU ARE NOT RUNNING LOCALLY\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your OpenAI API key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain tools\n",
    "\n",
    "Tools in LangChain are interfaces that allow an AI model (such as GPT-4) to interact with external systems, retrieve data, or perform actions beyond simple text generation. These tools act as APIs or function calls that the AI can invoke when needed.\n",
    "\n",
    "First, you'll create a tool to identify available datasets in the local directory.  This tool helps your agent discover what CSV files are available for analysis without requiring the user to explicitly specify filenames. The assumption is that the CSV files have descriptive names that indicate their content. This is typically the first step in your data analysis workflow: discovering what data is available before loading or analyzing anything.\n",
    "\n",
    "LangChain tools are simple Python functions wrapped with the `@tool` decorator (imported from `langchain_core.tools`). They allow Large Language Models (LLMs) to call specific functions, enabling structured workflows and external tool integrations.\n",
    "\n",
    "```python\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def my_tool(input: <type>) -> <output_type>:\n",
    "    \"\"\"\n",
    "    Short description of what the tool does.\n",
    "\n",
    "    Args:\n",
    "        input (<type>): Explanation of the input argument.\n",
    "\n",
    "    Returns:\n",
    "        <output_type>: Explanation of the returned value.\n",
    "    \"\"\"\n",
    "    # implement your logic here\n",
    "    return tool_output\n",
    "```\n",
    "\n",
    "The tool components:\n",
    "\n",
    "- **`@tool` Decorator:** Marks the function as a tool, allowing LangChain to integrate it and expose it to the LLM.\n",
    "\n",
    "- **Input Arguments:** The parameters your tool function accepts, along with type annotations for clarity.\n",
    "\n",
    "- **Tool Description:** A clear, concise explanation used by LangChain and the LLM to understand when and how to call the tool.\n",
    "\n",
    "- **Return Type:** Specifies the type of data your tool will return, improving clarity and reliability.\n",
    "\n",
    "- **`.name`:** Automatically derived from your Python function name; used by LangChain to identify the tool.\n",
    "\n",
    "- **`.description`:** Automatically extracted from your function's docstring; helps the LLM understand the toolâ€™s purpose.\n",
    "\n",
    "- **`.args`:** Represents input arguments with their associated types, allowing LangChain to validate and pass correct values to your tool function.\n",
    "\n",
    "Let's create the first LangChain tool, which lists all CSV files in the current directory.\n",
    "\n",
    "- `os.getcwd()` retrieves the current working directory.\n",
    "- `os.path.join(os.getcwd(), \"*.csv\")` constructs a path pattern to match all CSV files (`*` matches all filenames ending with `.csv`).\n",
    "- `glob.glob(pattern)` returns a list of files that match the given pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_csv_files() -> Optional[List[str]]:\n",
    "    \"\"\"List all CSV file names in the local directory.\n",
    "\n",
    "    Returns:\n",
    "        A list containing CSV file names.\n",
    "        If no CSV files are found, returns None.\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(os.getcwd(), \"*.csv\"))\n",
    "    if not csv_files:\n",
    "        return None\n",
    "    return [os.path.basename(file) for file in csv_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print out the useful attributes of the tool, which helps during debugging and allows LangChain to identify the purpose and inputs of each function clearly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: list_csv_files\n",
      "Tool Description: List all CSV file names in the local directory.\n",
      "\n",
      "    Returns:\n",
      "        A list containing CSV file names.\n",
      "        If no CSV files are found, returns None.\n",
      "Tool Arguments: {}\n"
     ]
    }
   ],
   "source": [
    "print(\"Tool Name:\", list_csv_files.name)\n",
    "print(\"Tool Description:\", list_csv_files.description)\n",
    "print(\"Tool Arguments:\", list_csv_files.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset caching tool\n",
    "\n",
    " As you build more complex tools, you need to efficiently manage datasets in memory. Since language models communicate via text, sending entire datasets in each response would waste tokens and context window space. To solve this, you'll create a global cache that stores DataFrames after they're first loaded. This approach has several benefits:\n",
    "   1. Reduces token usage by referencing datasets by name rather than content\n",
    "   2. Improves performance by loading data only once\n",
    "   3. Maintains dataset availability between different tool calls\n",
    "\n",
    " The following tool allows the agent to preload datasets into this cache system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_CACHE = {}\n",
    "\n",
    "@tool\n",
    "def preload_datasets(paths: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Loads CSV files into a global cache if not already loaded.\n",
    "    \n",
    "    This function helps to efficiently manage datasets by loading them once\n",
    "    and storing them in memory for future use. Without caching, you would\n",
    "    waste tokens describing dataset contents repeatedly in agent responses.\n",
    "    \n",
    "    Args:\n",
    "        paths: A list of file paths to CSV files.\n",
    "\n",
    "    Returns:\n",
    "        A message summarizing which datasets were loaded or already cached.\n",
    "    \"\"\"\n",
    "    loaded = []\n",
    "    cached = []\n",
    "    for path in paths:\n",
    "        if path not in DATAFRAME_CACHE:\n",
    "            DATAFRAME_CACHE[path] = pd.read_csv(path)\n",
    "            loaded.append(path)\n",
    "        else:\n",
    "            cached.append(path)\n",
    "    \n",
    "    return (\n",
    "        f\"Loaded datasets: {loaded}\\n\"\n",
    "        f\"Already cached: {cached}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may think that the `global` keyword would work effectively instead of `DATAFRAME CACHE`, but when using functions in Python with LangChain agents, simply writing the keyword `global` isn't enough to maintain data between different tool calls. This is because each time the agent runs a tool, it might do so in a separate execution environment, causing any `global` variables to reset. Instead, using a module-level dictionary (`DATAFRAME_CACHE`) that lives outside any function creates a persistent storage space that all tools can access without explicitly passing it around. This approach works reliably across multiple function calls, even when they happen in different contexts, and keeps the function interfaces clean by avoiding the need to pass the cache as an additional parameter to every tool.\n",
    " \n",
    " Well-structured docstrings are essential for your LLM-based tools because they serve as instructions for the AI agent. The agent relies on these descriptions to understand when and how to use each tool. Similarly, formatted outputs help the agent parse and interpret results properly. Always ensure your tool functions have clear docstrings and return well-structured outputs that are easy for both humans and AI to understand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization tool \n",
    "Next, create a tool to provide dataset summaries with key statistical information. This tool gives the agent a quick overview of each dataset without transferring the entire content. It examines the structure of each CSV file and returns metadata that helps the agent understand what kinds of data it's working with.\n",
    "\n",
    " Type annotations are extremely important in the tools for several reasons:\n",
    "   1. They provide clear documentation of expected inputs and outputs\n",
    "   2. They enable static type checking to catch errors before runtime\n",
    "   3. Most critically, they help the AI agent understand exactly what data to provide and what format to expect in return\n",
    "\n",
    "For this tool, nested type annotations (List[Dict[str, Any]]) precisely define the structured data that will be returned. This allows the agent to parse and utilize the results correctly in subsequent reasoning steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional,Dict,Any\n",
    "\n",
    "@tool\n",
    "def get_dataset_summaries(dataset_paths: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Analyze multiple CSV files and return metadata summaries for each.\n",
    "\n",
    "    Args:\n",
    "        dataset_paths (List[str]): \n",
    "            A list of file paths to CSV datasets.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: \n",
    "            A list of summaries, one per dataset, each containing:\n",
    "            - \"file_name\": The path of the dataset file.\n",
    "            - \"column_names\": A list of column names in the dataset.\n",
    "            - \"data_types\": A dictionary mapping column names to their data types (as strings).\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    for path in dataset_paths:\n",
    "        # Load and cache the dataset if not already cached\n",
    "        if path not in DATAFRAME_CACHE:\n",
    "            DATAFRAME_CACHE[path] = pd.read_csv(path)\n",
    "        \n",
    "        df = DATAFRAME_CACHE[path]\n",
    "\n",
    "        # Build summary\n",
    "        summary = {\n",
    "            \"file_name\": path,\n",
    "            \"column_names\": df.columns.tolist(),\n",
    "            \"data_types\": df.dtypes.astype(str).to_dict()\n",
    "        }\n",
    "\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame method execution tool\n",
    "\n",
    "Now that you have a basic understanding of your datasets, you need a flexible way to explore and analyze them. Just like data scientists use various pandas DataFrame methods (`head()`, `describe()`, `info()`, etc.), your agent needs the ability to apply these methods to your cached datasets.\n",
    "\n",
    "You'll leverage Python's `getattr()` function, which allows you to retrieve and call a method using its string name. This approach gives your agent the flexibility to select the most appropriate DataFrame method based on the current analysis needs.\n",
    "\n",
    "By providing both the file name and the method name as parameters, the LLM can intelligently choose which analysis techniques to apply to different datasets. \n",
    "The result of each method call is converted to a string representation, making it accessible to the LLM for further analysis and reasoning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def call_dataframe_method(file_name: str, method: str) -> str:\n",
    "   \"\"\"\n",
    "   Execute a method on a DataFrame and return the result.\n",
    "   This tool lets you run simple DataFrame methods like 'head', 'tail', or 'describe' \n",
    "   on a dataset that has already been loaded and cached using 'preload_datasets'.\n",
    "   Args:\n",
    "       file_name (str): The path or name of the dataset in the global cache.\n",
    "       method (str): The name of the method to call on the DataFrame. Only no-argument \n",
    "                     methods are supported (e.g., 'head', 'describe', 'info').\n",
    "   Returns:\n",
    "       str: The output of the method as a formatted string, or an error message if \n",
    "            the dataset is not found or the method is invalid.\n",
    "   Example:\n",
    "       call_dataframe_method(file_name=\"data.csv\", method=\"head\")\n",
    "   \"\"\"\n",
    "   # Try to get the DataFrame from cache, or load it if not already cached\n",
    "   if file_name not in DATAFRAME_CACHE:\n",
    "       try:\n",
    "           DATAFRAME_CACHE[file_name] = pd.read_csv(file_name)\n",
    "       except FileNotFoundError:\n",
    "           return f\"DataFrame '{file_name}' not found in cache or on disk.\"\n",
    "       except Exception as e:\n",
    "           return f\"Error loading '{file_name}': {str(e)}\"\n",
    "   \n",
    "   df = DATAFRAME_CACHE[file_name]\n",
    "   func = getattr(df, method, None)\n",
    "   if not callable(func):\n",
    "       return f\"'{method}' is not a valid method of DataFrame.\"\n",
    "   try:\n",
    "       result = func()\n",
    "       return str(result)\n",
    "   except Exception as e:\n",
    "       return f\"Error calling '{method}' on '{file_name}': {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation tools\n",
    "\n",
    "These tools provide specialized functionality for building and evaluating machine learning models on datasets. The agent will first analyze the dataset structure using previous tools to determine if the prediction task is classification or regression. Then, based on its assessment, it will call either `evaluate_classification_dataset` or `evaluate_regression_dataset`, providing the appropriate dataset filename and target column. Both tools handle the technical aspects of machine learning (splitting the data, training the model, and calculating performance metrics) while abstracting away implementation details. For classification tasks, the agent will examine the target column's data type and unique values to determine if it's categorical, then call the classification evaluator, which returns accuracy metrics. For regression tasks involving continuous numerical predictions, the agent will select the regression evaluator that returns the RÂ² score and mean squared error. This decision-making process showcases how a well-designed agent can choose the appropriate tool based on data characteristics, demonstrating intelligent workflow automation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assumes this global cache is shared\n",
    "DATAFRAME_CACHE = {}\n",
    "\n",
    "@tool\n",
    "def evaluate_classification_dataset(file_name: str, target_column: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Train and evaluate a classifier on a dataset using the specified target column.\n",
    "    Args:\n",
    "        file_name (str): The name or path of the dataset stored in DATAFRAME_CACHE.\n",
    "        target_column (str): The name of the column to use as the classification target.\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary with the model's accuracy score.\n",
    "    \"\"\"\n",
    "    # Try to get the DataFrame from cache, or load it if not already cached\n",
    "    if file_name not in DATAFRAME_CACHE:\n",
    "        try:\n",
    "            DATAFRAME_CACHE[file_name] = pd.read_csv(file_name)\n",
    "        except FileNotFoundError:\n",
    "            return {\"error\": f\"DataFrame '{file_name}' not found in cache or on disk.\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error loading '{file_name}': {str(e)}\"}\n",
    "    \n",
    "    df = DATAFRAME_CACHE[file_name]\n",
    "    if target_column not in df.columns:\n",
    "        return {\"error\": f\"Target column '{target_column}' not found in '{file_name}'.\"}\n",
    "    \n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "@tool\n",
    "def evaluate_regression_dataset(file_name: str, target_column: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Train and evaluate a regression model on a dataset using the specified target column.\n",
    "    Args:\n",
    "        file_name (str): The name or path of the dataset stored in DATAFRAME_CACHE.\n",
    "        target_column (str): The name of the column to use as the regression target.\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary with RÂ² score and Mean Squared Error.\n",
    "    \"\"\"\n",
    "    # Try to get the DataFrame from cache, or load it if not already cached\n",
    "    if file_name not in DATAFRAME_CACHE:\n",
    "        try:\n",
    "            DATAFRAME_CACHE[file_name] = pd.read_csv(file_name)\n",
    "        except FileNotFoundError:\n",
    "            return {\"error\": f\"DataFrame '{file_name}' not found in cache or on disk.\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error loading '{file_name}': {str(e)}\"}\n",
    "    \n",
    "    df = DATAFRAME_CACHE[file_name]\n",
    "    if target_column not in df.columns:\n",
    "        return {\"error\": f\"Target column '{target_column}' not found in '{file_name}'.\"}\n",
    "    \n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return {\n",
    "        \"r2_score\": r2,\n",
    "        \"mean_squared_error\": mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "Agents in LangChain are advanced components that enable AI models to decide when and how to use tools dynamically. Instead of relying on predefined scripts, agents analyze user queries and choose the best tools to achieve a goal. The next step is defining your agent, which requires specifying how it should think and behave. You'll use `ChatPromptTemplate.from_messages()` to create a structured prompt with three essential components:\n",
    "\n",
    "1. **System message**: This establishes the agent's identity and primary objective. You define it as a data science assistant whose task is to analyze CSV files and determine whether each dataset is suitable for classification or regression based on its structure. This gives the agent a clear purpose and scope.\n",
    "\n",
    "2. **User input**: The `{input}` placeholder will be replaced with the user's actual query. This allows the agent to respond directly to what the user is asking about.\n",
    "\n",
    "3. **Agent scratchpad**: The `{agent_scratchpad}` placeholder is crucial for tool-calling agents as it provides space for the agent to show its reasoning process and track intermediate steps. This enables the agent to build a chain of thought, call tools sequentially, and use the results from one tool to inform decisions about subsequent tool calls.\n",
    "\n",
    "![agents copy.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/TYkDvBmpmmSXx6TftNpJgw/agents%20copy.png)\n",
    "\n",
    "[Reference article for image](https://medium.com/@Shamimw/understanding-langchain-tools-and-agents-a-guide-to-building-smart-ai-applications-e81d200b3c12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool, create_openai_tools_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# ðŸ§  Step 2: Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are a data science assistant. Use the available tools to analyze CSV files. \"\n",
    "     \"Your job is to determine whether each dataset is for classification or regression, based on its structure.\"),\n",
    "    \n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")  # Required for tool-calling agents\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a chatbot object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", streaming=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list tool that contains all the tool objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[list_csv_files, preload_datasets, get_dataset_summaries, call_dataframe_method, evaluate_classification_dataset, evaluate_regression_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent creation and limitations\n",
    "\n",
    "Here, you'll create your agent using `create_openai_tools_agent()`, which combines the language model, tools, and prompt template into a functional agent. However, this raw agent has significant limitations when used directly. It only performs a single step of reasoning and tool usage per invocation, then returns its intermediate thought process rather than a final answer. This behavior occurs because the agent doesn't automatically manage the full loop of thinking, acting, observing results, and continuing to reason until reaching a complete solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tool calling agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\n",
    "    \"input\": \"Can you tell me about the dataset?\",\n",
    "    \"intermediate_steps\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Agent decided to call a tool:\n",
      "Tool Name: list_csv_files\n",
      "Tool Input: {}\n",
      "Log:\n",
      " Invoking: `list_csv_files` with `{}`\n"
     ]
    }
   ],
   "source": [
    "# Get the first ToolAgentAction from the list\n",
    "action = response[0]\n",
    "\n",
    "# Print the key details\n",
    "print(\"ðŸ§  Agent decided to call a tool:\")\n",
    "print(\"Tool Name:\", action.tool)\n",
    "print(\"Tool Input:\", action.tool_input)\n",
    "print(\"Log:\\n\", action.log.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the agent was called with the input \"Can you tell me about the dataset?\", it responded with a tool action: it chose to invoke the list_csv_files tool without any arguments. It didnâ€™t try to load or analyze the dataset \n",
    "\n",
    " ReAct-style agents follow a step-by-step reasoning loop. ReAct stands for Reasoning and Acting: the agent thinks about what to do next, takes one action (like calling a tool), then waits for the result before continuing. This is why the agent's first instinct is to gather contextâ€”by listing the available CSV filesâ€”before attempting anything more complex. This isnâ€™t a failure; itâ€™s how the agent is designed to operateâ€”reasoning one step at a time based on feedback. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent executor ReAct\n",
    "\n",
    "Managing this ReAct loop manually can be cumbersome, which is why you'll use the AgentExecutor. The AgentExecutor wraps the agent and the toolset, and handles the full tool-use loop behind the scenes. It automatically runs the agent, executes the selected tool, takes the result (observation), and feeds it back into the agent until a final answer is reached. Without the executor, you'd have to manually manage every step, including checking whether the agent returned a tool call or a final answer, running the tool, and tracking the intermediate stepsâ€”all of which the executor handles for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent executor configuration\n",
    "\n",
    "The `AgentExecutor` line creates a complete, autonomous agent system by wrapping your basic agent with additional functionality. This executor manages the full ReAct loop (reasoning and acting) that allows the agent to make multiple tool calls in sequence until reaching a final answer. Configure it with several important parameters:\n",
    "\n",
    "1. **agent**: The agent to run for creating a plan and determining actions to take at each step of the execution loop.\n",
    "\n",
    "2. **tools**: The valid tools the agent can call.\n",
    "3. **verbose=True**: Enables detailed logging of each step in the agent's thinking and tool-calling process, which is invaluable for debugging and understanding how the agent arrives at its conclusions.\n",
    "\n",
    "4. **handle_parsing_errors=True**: Rather than crashing, the executor will attempt to recover and continue the conversation.\n",
    "\n",
    "The second line, `agent_executor.agent.stream_runnable = False`, disables streaming mode for the agent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True,handle_parsing_errors=True)\n",
    "agent_executor.agent.stream_runnable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now build a bot DataWizard:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Ask questions about your dataset (type 'exit' to quit):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " You: what datasets do we have and what can we do with them today?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `list_csv_files` with `{}`\n",
      "\n",
      "\n",
      "\u001b[32;1m\u001b[1;3mWe have two datasets available:v', 'regression-dataset.csv']\u001b[0m\n",
      "\n",
      "1. **classification-dataset.csv**\n",
      "2. **regression-dataset.csv**\n",
      "\n",
      "With these datasets, we can analyze their structure, determine if they are suitable for classification or regression, and evaluate models accordingly. We can perform the following tasks:\n",
      "\n",
      "- Get summaries of the datasets, including column names and data types.\n",
      "- Train and evaluate classification models on the classification dataset.\n",
      "- Train and evaluate regression models on the regression dataset.\n",
      "\n",
      "Would you like to analyze the datasets now?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "my Agent: We have two datasets available:\n",
      "\n",
      "1. **classification-dataset.csv**\n",
      "2. **regression-dataset.csv**\n",
      "\n",
      "With these datasets, we can analyze their structure, determine if they are suitable for classification or regression, and evaluate models accordingly. We can perform the following tasks:\n",
      "\n",
      "- Get summaries of the datasets, including column names and data types.\n",
      "- Train and evaluate classification models on the classification dataset.\n",
      "- Train and evaluate regression models on the regression dataset.\n",
      "\n",
      "Would you like to analyze the datasets now?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " You: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello! How can I assist you today? If you need help with analyzing CSV files or any data science-related inquiries, just let me know!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "my Agent: Hello! How can I assist you today? If you need help with analyzing CSV files or any data science-related inquiries, just let me know!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " You: Get summaries of the datasets, including column names and data types.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `list_csv_files` with `{}`\n",
      "\n",
      "\n",
      "\u001b[32;1m\u001b[1;3m1;3m['classification-dataset.csv', 'regression-dataset.csv']\u001b[0m\n",
      "Invoking: `preload_datasets` with `{'paths': ['classification-dataset.csv', 'regression-dataset.csv']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mLoaded datasets: ['classification-dataset.csv', 'regression-dataset.csv']\n",
      "\u001b[32;1m\u001b[1;3md: []\u001b[0m\n",
      "Invoking: `get_dataset_summaries` with `{'dataset_paths': ['classification-dataset.csv', 'regression-dataset.csv']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m[{'file_name': 'classification-dataset.csv', 'column_names': ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension', 'target'], 'data_types': {'mean radius': 'float64', 'mean texture': 'float64', 'mean perimeter': 'float64', 'mean area': 'float64', 'mean smoothness': 'float64', 'mean compactness': 'float64', 'mean concavity': 'float64', 'mean concave points': 'float64', 'mean symmetry': 'float64', 'mean fractal dimension': 'float64', 'radius error': 'float64', 'texture error': 'float64', 'perimeter error': 'float64', 'area error': 'float64', 'smoothness error': 'float64', 'compactness error': 'float64', 'concavity error': 'float64', 'concave points error': 'float64', 'symmetry error': 'float64', 'fractal dimension error': 'float64', 'worst radius': 'float64', 'worst texture': 'float64', 'worst perimeter': 'float64', 'worst area': 'float64', 'worst smoothness': 'float64', 'worst compactness': 'float64', 'worst concavity': 'float64', 'worst concave points': 'float64', 'worst symmetry': 'float64', 'worst fractal dimension': 'float64', 'target': 'int64'}}, {'file_name': 'regression-dataset.csv', 'column_names': ['Unnamed: 0', 'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'target'], 'data_types': {'Unnamed: 0': 'int64', 'MedInc': 'float64', 'HouseAge': 'float64', 'AveRooms': 'float64', 'AveBedrms': 'float64', 'Population': 'float64', 'AveOccup': 'float64', 'Latitude': 'float64', 'Longitude': 'float64', 'target': 'float64'}}]\u001b[0m"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Ask questions about your dataset (type 'exit' to quit):\")\n",
    "\n",
    "while True:\n",
    "    user_input=input(\" You:\")\n",
    "    if user_input.strip().lower() in ['exit','quit']:\n",
    "        print(\"see ya later\")\n",
    "        break\n",
    "        \n",
    "    result=agent_executor.invoke({\"input\":user_input})\n",
    "    print(f\"my Agent: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Karan Goswami](https://author.skills.network/instructors/karan_goswami)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kunal Makwana](https://author.skills.network/instructors/kunal_makwana)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wilbur Elbouni](https://author.skills.network/instructors/wilbur_elbouni)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "7ce133a1a2969efb00d5da0eb4e9eff7891a1c925a212bc3db02e66aeb950729"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
